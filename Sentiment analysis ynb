{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dac4xppltzUH"
   },
   "source": [
    "# Sentiment Analysis using bag of words\n",
    "This model focuses completely on the words, or sometimes a string of words, but usually pays no attention to the \"context\" so-to-speak. The bag of words model usually has a large list, probably better thought of as a sort of \"dictionary,\" which are considered to be words that carry sentiment. These words each have their own \"value\" when found in text. The values are typically all added up and the result is a sentiment valuation. The equation to add and derive a number can vary, but this model mainly focuses on the words, and makes no attempt to actually understand language fundamentals.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r2P6jGeovmsc"
   },
   "source": [
    "## Importing necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "VMTDZFvwJm72",
    "outputId": "adeeef66-1de9-48ee-8b7a-fb2d018d151e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection  import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve, GridSearchCV\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from nltk import word_tokenize\n",
    "stop_words = stopwords.words('english')\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from nltk.stem import PorterStemmer\n",
    "pd.options.display.max_columns = 30\n",
    "%matplotlib inline\n",
    "from textblob import TextBlob, Word, Blobber\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup \n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection  import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve, GridSearchCV\n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import TextBlob, Word, Blobber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I5yyY-IVJm8n"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "path = '/Users/user/Downloads/project2/data/data_reviews'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "iw7JgQWuxsoN",
    "outputId": "6e94ef45-cb3d-4d05-ca6a-6fd3da7d508b"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection  import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import learning_curve, GridSearchCV\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve, log_loss\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from textblob import TextBlob, Word, Blobber\n",
    "from IPython.core import display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "import copy\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "stop_words = stopwords.words('english')\n",
    "pd.options.display.max_columns = 30\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CgLJN-PMwytZ"
   },
   "source": [
    "### Creating pandas dataframe of the dataset. \n",
    "#### We're importing x_train which stores the reviews, y_train holds the sentiment values, x_test are the reviews on which we will test our work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hqkImR-oJm85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2400"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = pd.read_csv(os.path.join(path,'x_test.csv'))\n",
    "x_train = pd.read_csv(os.path.join(path, 'x_train.csv'))\n",
    "y_train = pd.read_csv(os.path.join(path, 'y_train.csv'))\n",
    "y = y_train['is_positive_sentiment']\n",
    "y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZiY4wcpCydNy"
   },
   "source": [
    "On displaying the shape we now know that we have 2400 samples. Out of which 1200 are positive and 1200 are negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "2YIHoEIRJm_J",
    "outputId": "afb7474a-4c27-4970-a978-8659a767c450"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1200\n",
       "0    1200\n",
       "Name: is_positive_sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5lPkZYhyyyD5"
   },
   "source": [
    "### Few pre-processing techniques that could be employed are...\n",
    "*   Remove all punctuation from words.\n",
    "*   Remove all words that are not purely comprised of alphabetical characters.\n",
    "*   Remove all words that are not purely comprised of alphabetical characters.\n",
    "*   Remove all words that are known stop words.\n",
    "*   Remove all words that have a length <= 1 character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wYNlyxDK6Nc5"
   },
   "source": [
    "### Code for converting all the reviews to lower case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Convert into Lower case\n",
    "x_train[\"text\"] = (x_train[\"text\"].apply(lambda x: \" \".join(x.lower() for x in x.split())))\n",
    "x_test[\"text\"] = (x_test[\"text\"].apply(lambda x: \" \".join(x.lower() for x in x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing punctuation\n",
    "x_train[\"text\"] = x_train[\"text\"].str.replace('[^\\w\\s]','')\n",
    "x_test[\"text\"] = x_test[\"text\"].str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Removing stop words\n",
    "stop = stopwords.words('english')\n",
    "x_train[\"text\"] = x_train[\"text\"].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "x_test[\"text\"] = x_test[\"text\"].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###3Common words removal\n",
    "freq = pd.Series(' '.join(x_train['text']).split()).value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            oh forgot also mention weird color effect\n",
       "1                                    didnt work either\n",
       "2                                       waste 13 bucks\n",
       "3    product useless since enough charging current ...\n",
       "4        none three sizes sent headset would stay ears\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = list(freq.index)\n",
    "x_train['text'] = x_train['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "x_test['text'] = x_test['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "x_train['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Rare Word removal\n",
    "freq = pd.Series(' '.join(x_train['text']).split()).value_counts()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            oh forgot also mention weird color effect\n",
       "1                                    didnt work either\n",
       "2                                       waste 13 bucks\n",
       "3    product useless since enough charging current ...\n",
       "4        none three sizes sent headset would stay ears\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = list(freq.index)\n",
    "x_train['text'] = x_train['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "x_test['text'] = x_test['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "x_train['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q5jY8TkwJm_M"
   },
   "outputs": [],
   "source": [
    "###LOWER CASE############\n",
    "x_train[\"text\"] = (x_train[\"text\"].apply(lambda x: \" \".join(x.lower() for x in x.split())))\n",
    "x_test[\"text\"] = (x_test[\"text\"].apply(lambda x: \" \".join(x.lower() for x in x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            oh forgot also mention weird color effect\n",
       "1                                    didnt work either\n",
       "2                                        waste 13 buck\n",
       "3    product useless since enough charging current ...\n",
       "4          none three size sent headset would stay ear\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train['text'] = x_train['text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "x_test['text'] = x_test['text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "x_train['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['oh', 'forgot']),\n",
       " WordList(['forgot', 'also']),\n",
       " WordList(['also', 'mention']),\n",
       " WordList(['mention', 'weird']),\n",
       " WordList(['weird', 'color']),\n",
       " WordList(['color', 'effect'])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(x_train['text'][0]).ngrams(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['oh', 'forgot', 'also']),\n",
       " WordList(['forgot', 'also', 'mention']),\n",
       " WordList(['also', 'mention', 'weird']),\n",
       " WordList(['mention', 'weird', 'color']),\n",
       " WordList(['weird', 'color', 'effect'])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(x_train['text'][0]).ngrams(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization -> converting the word into root word\n",
    "st = PorterStemmer()\n",
    "x_train[\"text\"]  = (x_train[\"text\"].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()])))\n",
    "x_test[\"text\"]  = (x_test[\"text\"].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_words( raw_data ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    \n",
    "    # 1. Remove HTML\n",
    "    text = BeautifulSoup(raw_data).get_text() \n",
    "    \n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text) \n",
    "    \n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    \n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "     \n",
    "    # 5. Remove stop words\n",
    "    #meaningful_words = [w for w in words if not w in stops]   \n",
    "    meaningful_words = words\n",
    "    \n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_words( raw_data ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    \n",
    "    # 1. Remove HTML\n",
    "    text = BeautifulSoup(raw_data).get_text() \n",
    "    \n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text) \n",
    "    \n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    \n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "     \n",
    "    # 5. Remove stop words\n",
    "    #meaningful_words = [w for w in words if not w in stops]   \n",
    "    meaningful_words = words\n",
    "    \n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400\n"
     ]
    }
   ],
   "source": [
    "# Get the number of text based on the dataframe column size\n",
    "num_text = x_train['text'].size\n",
    "print (num_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text 1000 of 2400\n",
      "\n",
      "text 2000 of 2400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_df_text = []\n",
    "for i in range( 0, num_text ):\n",
    "    # If the index is evenly divisible by 1000, print a message\n",
    "    if((i+1)%1000 == 0):\n",
    "        print (\"text %d of %d\\n\" % ( i+1, num_text ) )                                                                   \n",
    "    clean_df_text.append( text_to_words( x_train[\"text\"][i] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "TLzSqwhH6arW",
    "outputId": "f0fbd478-3fd6-4d33-8387-166ed45b926e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>website_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>amazon</td>\n",
       "      <td>oh forgot also mention weird color effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>amazon</td>\n",
       "      <td>didnt work either</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>amazon</td>\n",
       "      <td>waste 13 buck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>amazon</td>\n",
       "      <td>product useless since enough charging current ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>amazon</td>\n",
       "      <td>none three size sent headset would stay ear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  website_name                                               text\n",
       "0       amazon          oh forgot also mention weird color effect\n",
       "1       amazon                                  didnt work either\n",
       "2       amazon                                      waste 13 buck\n",
       "3       amazon  product useless since enough charging current ...\n",
       "4       amazon        none three size sent headset would stay ear"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kTYeZ0dg6Z1q"
   },
   "source": [
    "### Removing all punctuation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "by63mO_jJm_P"
   },
   "outputs": [],
   "source": [
    "# Removing punctuation\n",
    "x_train[\"text\"] = x_train[\"text\"].str.replace('[^\\w\\s]','')\n",
    "x_test[\"text\"] = x_test[\"text\"].str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "cOah_K-w6rjI",
    "outputId": "41cae355-5b52-4cee-c4f2-f28be1c7060c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>website_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>amazon</td>\n",
       "      <td>oh forgot also mention weird color effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>amazon</td>\n",
       "      <td>didnt work either</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>amazon</td>\n",
       "      <td>waste 13 buck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>amazon</td>\n",
       "      <td>product useless since enough charging current ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>amazon</td>\n",
       "      <td>none three size sent headset would stay ear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  website_name                                               text\n",
       "0       amazon          oh forgot also mention weird color effect\n",
       "1       amazon                                  didnt work either\n",
       "2       amazon                                      waste 13 buck\n",
       "3       amazon  product useless since enough charging current ...\n",
       "4       amazon        none three size sent headset would stay ear"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c2jYJoGl6vPo"
   },
   "source": [
    "### Lemmatizing all reviews in order to group similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yKhobiH_Jm_T"
   },
   "outputs": [],
   "source": [
    "####lEMENTIZATIOO#############\n",
    "x_train[\"text\"]  = (x_train[\"text\"].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()])))\n",
    "x_test[\"text\"]  = (x_test[\"text\"].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "52gbO2kt7ZM8",
    "outputId": "5aecc1dc-9f39-4735-d2cc-df8711f81da1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>website_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>amazon</td>\n",
       "      <td>oh forgot also mention weird color effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>amazon</td>\n",
       "      <td>didnt work either</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>amazon</td>\n",
       "      <td>waste 13 buck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>amazon</td>\n",
       "      <td>product useless since enough charging current ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>amazon</td>\n",
       "      <td>none three size sent headset would stay ear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  website_name                                               text\n",
       "0       amazon          oh forgot also mention weird color effect\n",
       "1       amazon                                  didnt work either\n",
       "2       amazon                                      waste 13 buck\n",
       "3       amazon  product useless since enough charging current ...\n",
       "4       amazon        none three size sent headset would stay ear"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "plMh0n85vOH4"
   },
   "source": [
    "## Stopword elimination\n",
    "*  Stopwords are words that occur wayy too many times in the given corpus.\n",
    "*  Stopwords are not helpful or healthy for any alg because we will be wasting time and resource on training them when no o/p can be expected\n",
    "*  Stopwords lists readily available online from sources such as sklearn etc cannot be used all the time.\n",
    "*  They cannot be used all the time because, the range of stopwords and their frequency differs from corpus to corpus.\n",
    "*  Hence it is a healthy to check for the word frequency in the corpus before eliminating all stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VpAkh55ezL9A"
   },
   "source": [
    "### Multiclass logloss\n",
    "Multi class version of Logarithmic Loss metric.\n",
    "    :param actual: Array containing the actual target classes\n",
    "    :param predicted: Matrix with class predictions, one probability per class\n",
    "    Log loss, aka logistic loss or cross-entropy loss.\n",
    "    This is the loss function used in (multinomial) logistic regression and \n",
    "    extensions of it such as neural networks, defined as the negative log-likelihood of the true labels \n",
    "    given a probabilistic classifier’s predictions. The log loss is only defined for two or more labels.\n",
    "    Accuracy is the count of predictions where your predicted value equals the actual value. \n",
    "    Accuracy is not always a good indicator because of its yes or no nature.\n",
    "    Log Loss takes into account the uncertainty of your prediction based on how much it varies from the actual label. \n",
    "    This gives us a more nuanced view into the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PKcKlsU2Jm_Z"
   },
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vY4-GTL0qO4z"
   },
   "source": [
    "## Important Information regarding the Pre-Processing\n",
    "*  We've only been tokenizing ngrams in the range (1,1), that means we have separated each and every word without bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rJM4EU_tzVQ5"
   },
   "source": [
    "### Splitting out data to train and test sets\n",
    "*  Here we're dividing the dataset into 4 parts being training set and the validation set.\n",
    "*  Notice how validation set is different from testing set.\n",
    "*  We're shuffling the data as we would want the reviews from amazon, yelp and imdb to well mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jfxv0Vz5Jm_c"
   },
   "outputs": [],
   "source": [
    "#xtrain, xvalid, ytrain, yvalid = train_test_split(x_train['text'], y \n",
    "#                                                  , stratify=y\n",
    "#                                                  ,random_state=42, \n",
    "#                                                  test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train = x_train['text']\n",
    "#x_test = x_test['text']\n",
    "#y_train = y_train['is_positive_sentiment']\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(x_train['text'], y_train['is_positive_sentiment'] \n",
    "                                                  , stratify=y_train['is_positive_sentiment']\n",
    "                                                  ,random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Zkw_q3wZJm_h",
    "outputId": "b5c46fde-ceff-4e5a-8733-f79f78f0787b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2160,)\n",
      "(240,)\n"
     ]
    }
   ],
   "source": [
    "print (xtrain.shape)\n",
    "print (xvalid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AUW5HFArwzrz"
   },
   "source": [
    "## Intuition on vectorization\n",
    "*  The simplest scoring method is to mark the presence of words as a boolean value, 0 for absent, 1 for present.\n",
    "\n",
    "*  Using the arbitrary ordering of words listed above in our vocabulary, we can step through the first document (“It was the best of times“) and convert it into a binary vector.\n",
    "\n",
    "*  The scoring of the document would look as follows:\n",
    "consider the example, \n",
    "\"\"\n",
    "*  “it” = 1\n",
    "*  “was” = 1\n",
    "*  “the” = 1\n",
    "*  “best” = 1\n",
    "*  “of” = 1\n",
    "*  “times” = 1\n",
    "*  “worst” = 0\n",
    "*  “age” = 0\n",
    "*  “wisdom” = 0\n",
    "*  “foolishness” = 0\n",
    "*  As a binary vector, this would look as follows:\n",
    "\n",
    "*  \"it was the worst of times\" = [1, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
    "*  \"it was the age of wisdom\" = [1, 1, 1, 0, 1, 0, 0, 1, 1, 0]\n",
    "*  \"it was the age of foolishness\" = [1, 1, 1, 0, 1, 0, 0, 1, 0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ju0FZ3LwJ84G"
   },
   "source": [
    "## Why vectorize?\n",
    "Before we can use words in a classifier, we need to convert them into numbers. One way to do that is to simply map words to integers. Another way is to one-hot encode words. Each tweet could then be represented as a vector with a dimension equal to (a limited set of) the words in the corpus. The words occurring in the tweet have a value of 1 in the vector. All other vector values equal zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BFn65xC0zoKp"
   },
   "source": [
    "### TF-IDF vector creation and fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5QWHE30NJm__"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2160, 1051), (240, 1051), (600, 1051))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######tfidf vector Creation##############\n",
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "\n",
    "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
    "tfv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_tfv =  tfv.transform(xtrain) \n",
    "xvalid_tfv = tfv.transform(xvalid)\n",
    "xtest_tfv = tfv.transform(x_test['text'])\n",
    "xtrain_tfv.shape, xvalid_tfv.shape, xtest_tfv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N3Nui1yUsPZu"
   },
   "source": [
    "*  If careful observation, we have set the ngram_range parameter to be (1, 3). That means the vectorizer is open to \"unigram\", \"bigram\" and \"trigram\" words. \n",
    "*  The same range has been specified at the countvectorizer too..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xJK9AVAszzPU"
   },
   "source": [
    "### Count Vector creation and fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-0cL1VriJnAU"
   },
   "outputs": [],
   "source": [
    "######Count vector Creation##############\n",
    "ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), stop_words = 'english')\n",
    "\n",
    "# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\n",
    "ctv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_ctv =  ctv.transform(xtrain) \n",
    "xvalid_ctv = ctv.transform(xvalid)\n",
    "xtest_ctv = ctv.transform(x_test['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jWCkGTGruGA4"
   },
   "source": [
    "# Training Process\n",
    "## The details of the training are as follows\n",
    "*  Logistic regression over tfidf\n",
    "*  Logistic regression over ctv\n",
    "*  Naive Baye's over tfidv\n",
    "*  Naive Baye's over ctv\n",
    "*  Random forest over tfidfv\n",
    "*  Random forest over ctv\n",
    "*  Grid search for optimal hyper parameters for Logistic regression, Naive Baye's and Random forest algorithms \n",
    "*  Comparative study was made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2aJSveZg5FjY"
   },
   "source": [
    "## Logistic Regression\n",
    "*  By default sklearn employs a penalty of \"l2\" when we call for logistic regression.\n",
    "*  Logistic regression is the right algorithm to start with classification algorithms. Eventhough, the name ‘Regression’ comes up, it is not a regression model, but a classification model. It uses a logistic function to frame binary output model. The output of the logistic regression will be a probability (0≤x≤1), and can be used to predict the binary 0 or 1 as the output ( if x<0.5, output= 0, else output=1).\n",
    "*  Cross entropy is used as the loss function in Logistic regression\n",
    "*  θ parameters explains the direction and intensity of significance\n",
    "independent variables over the dependent variable.\n",
    "*  Can be used for multiclass classifications also.\n",
    "*  Loss function is always convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GzvhKrJIz6DR"
   },
   "source": [
    "### Fitting a Logistic Regression Model over tfidf vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "kVP6GKl7JnAp",
    "outputId": "114975a1-b8cb-4206-c019-2e8030b8311b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.532 \n",
      "Accuracy for Logistic regression on Test TFIDF vector:  0.7583333333333333\n",
      "Accuracy for Logistic regression on Training TFIDF vector:  0.8814814814814815\n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Logistic Regression on TFIDF#########\n",
    "LR_IDF = LogisticRegression(C=1.0,solver='lbfgs')\n",
    "LR_IDF.fit(xtrain_tfv, ytrain)\n",
    "predictions = LR_IDF.predict_proba(xvalid_tfv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n",
    "y_pred_Linear = LR_IDF.predict(xvalid_tfv)\n",
    "\n",
    "print(\"Accuracy for Logistic regression on Test TFIDF vector: \",accuracy_score(yvalid, y_pred_Linear))\n",
    "pred_train=LR_IDF.predict(xtrain_tfv)\n",
    "print(\"Accuracy for Logistic regression on Training TFIDF vector: \",accuracy_score(ytrain,pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### uncertainty plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5, 1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "C_param_range = [0.001,0.01,0.1,1,10,100,1000]\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Apply logistic regression model to training data\n",
    "lr = LogisticRegression(penalty='l2',C = 1.0,random_state = 0)\n",
    "\n",
    "# SEPAL Plot validation curve\n",
    "train_scores, test_scores = validation_curve(estimator=lr\n",
    "                                                            ,X=xtrain_tfv\n",
    "                                                            ,y=ytrain\n",
    "                                                            ,param_name='C'\n",
    "                                                            ,param_range=C_param_range\n",
    "                                                            )\n",
    "\n",
    "train_mean = np.mean(train_scores,axis=1)\n",
    "train_std = np.std(train_scores,axis=1)\n",
    "test_mean = np.mean(test_scores,axis=1)\n",
    "test_std = np.std(test_scores,axis=1)\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(C_param_range\n",
    "            ,train_mean\n",
    "            ,color='blue'\n",
    "            ,marker='o'\n",
    "            ,markersize=5\n",
    "            ,label='training accuracy')\n",
    "    \n",
    "plt.plot(C_param_range\n",
    "            ,test_mean\n",
    "            ,color='green'\n",
    "            ,marker='x'\n",
    "            ,markersize=5\n",
    "            ,label='test accuracy') \n",
    "    \n",
    "plt.xlabel('C_parameter')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim([0.5,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F2QV1qFp0OZS"
   },
   "source": [
    "### Fitting a Logistic Regression Model over count vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "1UkvXyn1JnA9",
    "outputId": "b85b290b-eff1-45a1-846b-073a27229e52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.49 \n",
      "Accuracy for logistic regression on Test  Count Vector : 0.7791666666666667\n",
      "Accuracy for logistic regression on Training  Count Vector :  0.9837962962962963\n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Logistic Regression on Counts\n",
    "LR_Count = LogisticRegression(C=1.0,solver='lbfgs')\n",
    "LR_Count.fit(xtrain_ctv, ytrain)\n",
    "predictions = LR_Count.predict_proba(xvalid_ctv)\n",
    "\n",
    "print (\"logloss: %0.2f \" % multiclass_logloss(yvalid, predictions))\n",
    "y_pred_Linear = LR_Count.predict(xvalid_ctv)\n",
    "###print(y_pred_Linear)\n",
    "print(\"Accuracy for logistic regression on Test  Count Vector :\",accuracy_score(yvalid, y_pred_Linear))\n",
    "\n",
    "pred_train=LR_Count.predict(xtrain_ctv)\n",
    "print(\"Accuracy for logistic regression on Training  Count Vector : \",accuracy_score(ytrain,pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a simple Logistic Regression on TFIDF#########\n",
    "def grid_search_best_params(model_name, pipeline, params, xtrain, ytrain, xvalid, yvalid, X, Y):\n",
    "    grid = GridSearchCV(pipeline, param_grid=params, cv=5, n_jobs=-1,refit='accuracy', scoring=['accuracy','neg_log_loss','roc_auc'])\n",
    "    grid.fit(X, Y)\n",
    "    predictions = grid.predict_proba(xvalid)\n",
    "    grids[model_name] = copy.deepcopy(grid)\n",
    "    #print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n",
    "    y_pred_test = grid.predict(xvalid)\n",
    "    y_test_proba = grid.predict_proba(xvalid)\n",
    "    vec_type = 'TF-IDF Vectorizer' if 'TF-IDF' in model_name else 'CountVectorizer'\n",
    "    print(\"Log Loss for %s on Test %s : %s\"%(model_name,vec_type,log_loss(yvalid, y_pred_test)))\n",
    "    print(\"ROC AUC for %s on Test %s : %s\"%(model_name,vec_type,roc_auc_score(yvalid, y_test_proba.T[1])))\n",
    "    print(\"Accuracy for %s on Test %s : %s\"%(model_name,vec_type,accuracy_score(yvalid, y_pred_test)))\n",
    "    y_pred_train=grid.predict(xtrain)\n",
    "    y_train_proba = grid.predict_proba(xtrain)\n",
    "    print(\"Log Loss for %s on Training %s : %s\"%(model_name,vec_type,log_loss(ytrain, y_pred_train)))\n",
    "    print(\"ROC AUC for %s on Training %s : %s\"%(model_name,vec_type,roc_auc_score(ytrain, y_train_proba.T[1])))\n",
    "    print(\"Accuracy for %s on Training %s : %s\"%(model_name, vec_type,accuracy_score(ytrain,y_pred_train)))\n",
    "    print('Best Score : %.2f'%(grid.best_score_*100))\n",
    "    print('Best Paramerter Settings : ',grid.best_params_)\n",
    "    #print(\"ROC Area Under Curve in % : \",cross_val_score(pipeline_lr, X, Y, scoring=\"roc_auc\", cv=5)*100)\n",
    "    conf_mat = confusion_matrix(yvalid, y_pred_test)\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    sns.heatmap(conf_mat, annot=True, fmt='d')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show()\n",
    "    accuracies = {'Train Accuracy':accuracy_score(ytrain,y_pred_train), \n",
    "                  'Test Accuracy':accuracy_score(yvalid, y_pred_test),\n",
    "                  'Best Accuracy': grid.best_score_,\n",
    "                  'Train Log Loss':log_loss(ytrain, y_pred_train), \n",
    "                  'Test Log Loss':log_loss(yvalid, y_pred_test),\n",
    "                  'Train ROC AUC': roc_auc_score(ytrain, y_train_proba.T[1]), \n",
    "                  'Test ROC AUC': roc_auc_score(yvalid, y_test_proba.T[1]),\n",
    "                  'Best Param Settings' :grid.best_params_}\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = {}\n",
    "accuracies = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss for LogisticRegresion(TF-IDF) on Test TF-IDF Vectorizer : 2.302621741209597\n",
      "ROC AUC for LogisticRegresion(TF-IDF) on Test TF-IDF Vectorizer : 0.9725694444444445\n",
      "Accuracy for LogisticRegresion(TF-IDF) on Test TF-IDF Vectorizer : 0.9333333333333333\n",
      "Log Loss for LogisticRegresion(TF-IDF) on Training TF-IDF Vectorizer : 2.0787433837011493\n",
      "ROC AUC for LogisticRegresion(TF-IDF) on Training TF-IDF Vectorizer : 0.9773199588477366\n",
      "Accuracy for LogisticRegresion(TF-IDF) on Training TF-IDF Vectorizer : 0.9398148148148148\n",
      "Best Score : 74.79\n",
      "Best Paramerter Settings :  {'logisticregression__C': 1, 'logisticregression__penalty': 'l2', 'tfidfvectorizer__ngram_range': (1, 1)}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#min_df=3\n",
    "params_lr = {'logisticregression__C': [1],\n",
    "             'logisticregression__penalty': ['l1','l2'],\n",
    "          \"tfidfvectorizer__ngram_range\": [(1, 1), (1, 2), (2, 2), (1, 3), (2,3), (3,3)]}\n",
    "pipeline_lr = make_pipeline(TfidfVectorizer(max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english'), LogisticRegression())\n",
    "\n",
    "accuracies['LogisticRegresion(TF-IDF)'] = grid_search_best_params('LogisticRegresion(TF-IDF)',pipeline_lr, params_lr,xtrain,ytrain,xvalid,yvalid,x_train['text'],y_train['is_positive_sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss for LogisticRegresion(CV) on Test CountVectorizer : 0.7195711681844766\n",
      "ROC AUC for LogisticRegresion(CV) on Test CountVectorizer : 0.995625\n",
      "Accuracy for LogisticRegresion(CV) on Test CountVectorizer : 0.9791666666666666\n",
      "Log Loss for LogisticRegresion(CV) on Training CountVectorizer : 0.6556071395009052\n",
      "ROC AUC for LogisticRegresion(CV) on Training CountVectorizer : 0.9979389574759946\n",
      "Accuracy for LogisticRegresion(CV) on Training CountVectorizer : 0.9810185185185185\n",
      "Best Score : 74.83\n",
      "Best Paramerter Settings :  {'countvectorizer__ngram_range': (1, 2), 'logisticregression__C': 1, 'logisticregression__penalty': 'l2'}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "params_lr = {'logisticregression__C': [1],\n",
    "             'logisticregression__penalty': ['l1','l2'],\n",
    "          \"countvectorizer__ngram_range\": [(1, 1), (1, 2), (2, 2), (1, 3), (2,3), (3,3)]}\n",
    "pipeline_lr = make_pipeline(CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            stop_words = 'english'), LogisticRegression())\n",
    "\n",
    "accuracies['LogisticRegresion(CV)'] = grid_search_best_params('LogisticRegresion(CV)',pipeline_lr, params_lr,xtrain,ytrain,xvalid,yvalid,x_train['text'],y_train['is_positive_sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURAL NETWORK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conf_matrix(conf_mat):\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    sns.heatmap(conf_mat, annot=True, fmt='d')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show()\n",
    "\n",
    "def grid_search_best_params(model_name, pipeline, params, xtrain, ytrain, xvalid, yvalid):\n",
    "    grid = GridSearchCV(pipeline, param_grid=params, cv=5, n_jobs=1, refit='accuracy', scoring=['accuracy'])\n",
    "    grid.fit(xtrain, ytrain)\n",
    "    grids[model_name] = copy.deepcopy(grid)\n",
    "    y_pred_test = grid.predict(xvalid)\n",
    "    y_test_proba = grid.predict_proba(xvalid)\n",
    "    \n",
    "    vec_type = 'TF-IDF Vectorizer' if 'TF-IDF' in model_name else 'CountVectorizer'\n",
    "    print(\"\\nLog Loss for %s on Test Set for %s : %s\"%(model_name,vec_type,log_loss(yvalid, y_pred_test)))\n",
    "    print(\"ROC AUC  for %s on Test Set for %s : %s\"%(model_name,vec_type,roc_auc_score(yvalid, y_test_proba.T[1])))\n",
    "    print(\"Accuracy for %s on Test Set for %s : %s\"%(model_name,vec_type,accuracy_score(yvalid, y_pred_test)))\n",
    "    \n",
    "    \n",
    "    y_pred_train=grid.predict(xtrain)\n",
    "    y_train_proba = grid.predict_proba(xtrain)\n",
    "    print(\"Log Loss for %s on Training %s : %s\"%(model_name,vec_type,log_loss(ytrain, y_pred_train)))\n",
    "    print(\"ROC AUC for %s on Training %s : %s\"%(model_name,vec_type,roc_auc_score(ytrain, y_train_proba.T[1])))\n",
    "    print(\"Accuracy for %s on Training %s : %s\"%(model_name, vec_type,accuracy_score(ytrain,y_pred_train)))\n",
    " \n",
    "    print('Best Score : %.3f%%'%(grid.best_score_*100))\n",
    "    print('Best Paramerter Settings : ',grid.best_params_)\n",
    "    \n",
    "      \n",
    "    conf_mat = confusion_matrix(yvalid, y_pred_test)\n",
    "    plot_conf_matrix(conf_mat)\n",
    "    \n",
    "    accuracies = {'Val ROC AUC': roc_auc_score(yvalid, y_test_proba.T[1]),\n",
    "                  'Val Log Loss':log_loss(yvalid, y_pred_test),\n",
    "                  'Val Accuracy':accuracy_score(yvalid, y_pred_test),               \n",
    "                  'Best Accuracy': grid.best_score_,\n",
    "                  'Best Param Settings' :grid.best_params_,\n",
    "                  }\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Log Loss for NeuralNet(TF-IDF) on Test Set for TF-IDF Vectorizer : 9.642\n",
      "ROC AUC  for NeuralNet(TF-IDF) on Test Set for TF-IDF Vectorizer : 0.828\n",
      "Accuracy for NeuralNet(TF-IDF) on Test Set for TF-IDF Vectorizer : 72.083%\n",
      "Log Loss for NeuralNet(TF-IDF) on Training TF-IDF Vectorizer : 1.1193366301269179\n",
      "ROC AUC for NeuralNet(TF-IDF) on Training TF-IDF Vectorizer : 0.9975934499314129\n",
      "Accuracy for NeuralNet(TF-IDF) on Training TF-IDF Vectorizer : 0.9675925925925926\n",
      "Best Score : 71.852%\n",
      "Best Paramerter Settings :  {'mlpclassifier__activation': 'relu', 'mlpclassifier__hidden_layer_sizes': (64, 128, 256), 'tfidfvectorizer__ngram_range': (1, 1)}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "params_lr = {'mlpclassifier__hidden_layer_sizes': [(32, 64, 128) ,(64, 128, 256)],\n",
    "             'mlpclassifier__activation': ['relu'],\n",
    "             'tfidfvectorizer__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 2)]}\n",
    "pipeline_lr = make_pipeline(TfidfVectorizer(min_df=3,  max_features=None, strip_accents='unicode', analyzer='word',\n",
    "                                            token_pattern=r'\\w{1,}',use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "                                            stop_words = 'english'), \n",
    "                            MLPClassifier())\n",
    "\n",
    "accuracies['NeuralNet(TF-IDF)'] = grid_search_best_params('NeuralNet(TF-IDF)',pipeline_lr, params_lr,xtrain,ytrain,xvalid,yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Log Loss for NeuralNet(CV) on Test Set for CountVectorizer : 8.059\n",
      "ROC AUC  for NeuralNet(CV) on Test Set for CountVectorizer : 0.874\n",
      "Accuracy for NeuralNet(CV) on Test Set for CountVectorizer : 76.667%\n",
      "Log Loss for NeuralNet(CV) on Training CountVectorizer : 0.28782683846421125\n",
      "ROC AUC for NeuralNet(CV) on Training CountVectorizer : 0.9998113854595336\n",
      "Accuracy for NeuralNet(CV) on Training CountVectorizer : 0.9916666666666667\n",
      "Best Score : 75.278%\n",
      "Best Paramerter Settings :  {'countvectorizer__ngram_range': (1, 2), 'mlpclassifier__activation': 'relu', 'mlpclassifier__hidden_layer_sizes': (64, 128, 256)}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "params_lr = {'mlpclassifier__hidden_layer_sizes': [(32, 64, 128) ,(64, 128, 256)],\n",
    "             'mlpclassifier__activation': ['relu'],\n",
    "             'countvectorizer__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 2)]}\n",
    "pipeline_lr = make_pipeline(CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}', stop_words = 'english'), \n",
    "                            MLPClassifier())\n",
    "\n",
    "accuracies['NeuralNet(CV)'] = grid_search_best_params('NeuralNet(CV)',pipeline_lr, params_lr,xtrain,ytrain,xvalid,yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lr = {'mlpclassifier__hidden_layer_sizes': [(32, 64, 128) ,(64, 128, 256)],\n",
    "             'mlpclassifier__activation': ['logistic', 'tanh', 'relu'],\n",
    "             'mlpclassifier__alpha' : [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "             'tfidfvectorizer__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 2)]}\n",
    "pipeline_lr = make_pipeline(TfidfVectorizer(min_df=3,  max_features=None, strip_accents='unicode', analyzer='word',\n",
    "                                            token_pattern=r'\\w{1,}',use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "                                            stop_words = 'english'), \n",
    "                            MLPClassifier())\n",
    "\n",
    "accuracies['NeuralNet(TF-IDF)'] = grid_search_best_params('NeuralNet(TF-IDF)',pipeline_lr, params_lr,xtrain,ytrain,xvalid,yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lr = {'mlpclassifier__hidden_layer_sizes': [(32, 64, 128) ,(64, 128, 256)],\n",
    "             'mlpclassifier__activation': ['logistic', 'tanh', 'relu'],\n",
    "             'mlpclassifier__alpha' : [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "             'countvectorizer__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 2)]}\n",
    "pipeline_lr = make_pipeline(CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}', stop_words = 'english'), \n",
    "                            MLPClassifier())\n",
    "\n",
    "accuracies['NeuralNet(CV)'] = grid_search_best_params('NeuralNet(CV)',pipeline_lr, params_lr,xtrain,ytrain,xvalid,yvalid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J9EqU5tl5izm"
   },
   "source": [
    "## Random forests \n",
    "*  Random forests is a decision tree based algorithm used to solve regression and classification problems. An inverted tree is framed which is branched off from a homogeneous probability distributed root node, to highly heterogeneous leaf nodes, for deriving the output. \n",
    "*  Decision tree handles colinearity better than LR.\n",
    "*  Decision trees cannot derive the significance of features, but LR can.\n",
    "*  Decision trees are better for categorical values than LR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.045 \n",
      "Accuracy for Random Forest on Test TFIDF vector:  0.7333333333333333\n",
      "Accuracy for Random Forest on Training TFIDF vector:  0.9546296296296296\n"
     ]
    }
   ],
   "source": [
    "# # Fitting random forest  on TFIDF\n",
    "##from sklearn.ensemble import RandomForestClassifier\n",
    "rf_IDF = RandomForestClassifier(n_estimators = 10, random_state = 42)\n",
    "rf_IDF.fit(xtrain_tfv, ytrain)\n",
    "predictions = rf_IDF.predict_proba(xvalid_tfv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n",
    "\n",
    "y_pred =rf_IDF.predict(xvalid_tfv)\n",
    "##print (y_pred)\n",
    "print(\"Accuracy for Random Forest on Test TFIDF vector: \",accuracy_score(yvalid, y_pred_Linear))\n",
    "pred_train=rf_IDF.predict(xtrain_tfv)\n",
    "print(\"Accuracy for Random Forest on Training TFIDF vector: \",accuracy_score(ytrain,pred_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 1.038 \n",
      "Accuracy for Random Forest on Test  Count Vector : 0.7333333333333333\n",
      "Accuracy for Random Forest on Training Count Vector:  0.9805555555555555\n"
     ]
    }
   ],
   "source": [
    "# # Fitting random forest  on count\n",
    "rf_Count = RandomForestClassifier(n_estimators = 10, random_state = 42)\n",
    "rf_Count.fit(xtrain_ctv, ytrain)\n",
    "predictions = rf_Count.predict_proba(xvalid_ctv)\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n",
    "y_pred_Linear = rf_Count.predict(xvalid_ctv)\n",
    "\n",
    "##print(y_pred_Linear)\n",
    "print(\"Accuracy for Random Forest on Test  Count Vector :\",accuracy_score(yvalid, y_pred_Linear))\n",
    "\n",
    "pred_train=rf_Count.predict(xtrain_ctv)\n",
    "print(\"Accuracy for Random Forest on Training Count Vector: \",accuracy_score(ytrain,pred_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9LMQLObo0aTY"
   },
   "source": [
    "### Fitting a Random Forest over tfidf vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a Random Forest over TFIDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train['text']\n",
    "x_test = x_test['text']\n",
    "y_train = y_train['is_positive_sentiment']\n",
    "xtrain1, xvalid, ytrain1, yvalid = train_test_split(x_train, y_train \n",
    "                                                  , stratify=y_train\n",
    "                                                  ,random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2160, 1051), (240, 1051))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######tfidf vector Creation##############\n",
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "\n",
    "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
    "tfv.fit(list(xtrain1) + list(xvalid))\n",
    "xtrain_tfv =  tfv.transform(xtrain1) \n",
    "xvalid_tfv = tfv.transform(xvalid)\n",
    "xtrain_tfv.shape,xvalid_tfv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "######Count vector Creation##############\n",
    "ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), stop_words = 'english')\n",
    "\n",
    "# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\n",
    "ctv.fit(list(xtrain1) + list(xvalid))\n",
    "xtrain_ctv =  ctv.transform(xtrain1) \n",
    "xvalid_ctv = ctv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a simple Logistic Regression on TFIDF#########\n",
    "def grid_search_best_params(model_name, pipeline, params, xtrain, ytrain, xvalid, yvalid, X, Y):\n",
    "    grid = GridSearchCV(pipeline, param_grid=params, cv=5, n_jobs=-1,refit='accuracy', scoring=['accuracy','neg_log_loss','roc_auc'])\n",
    "    grid.fit(X, Y)\n",
    "    predictions = grid.predict_proba(xvalid)\n",
    "    grids[model_name] = copy.deepcopy(grid)\n",
    "    #print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n",
    "    y_pred_test = grid.predict(xvalid)\n",
    "    y_test_proba = grid.predict_proba(xvalid)\n",
    "    vec_type = 'TF-IDF Vectorizer' if 'TF-IDF' in model_name else 'CountVectorizer'\n",
    "    print(\"Log Loss for %s on Test %s : %s\"%(model_name,vec_type,log_loss(yvalid, y_pred_test)))\n",
    "    print(\"ROC AUC for %s on Test %s : %s\"%(model_name,vec_type,roc_auc_score(yvalid, y_test_proba.T[1])))\n",
    "    print(\"Accuracy for %s on Test %s : %s\"%(model_name,vec_type,accuracy_score(yvalid, y_pred_test)))\n",
    "    y_pred_train=grid.predict(xtrain)\n",
    "    y_train_proba = grid.predict_proba(xtrain)\n",
    "    print(\"Log Loss for %s on Training %s : %s\"%(model_name,vec_type,log_loss(ytrain, y_pred_train)))\n",
    "    print(\"ROC AUC for %s on Training %s : %s\"%(model_name,vec_type,roc_auc_score(ytrain, y_train_proba.T[1])))\n",
    "    print(\"Accuracy for %s on Training %s : %s\"%(model_name, vec_type,accuracy_score(ytrain,y_pred_train)))\n",
    "    print('Best Score : %.2f'%(grid.best_score_*100))\n",
    "    print('Best Paramerter Settings : ',grid.best_params_)\n",
    "    #print(\"ROC Area Under Curve in % : \",cross_val_score(pipeline_lr, X, Y, scoring=\"roc_auc\", cv=5)*100)\n",
    "    conf_mat = confusion_matrix(yvalid, y_pred_test)\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    sns.heatmap(conf_mat, annot=True, fmt='d')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show()\n",
    "    accuracies = {'Train Accuracy':accuracy_score(ytrain,y_pred_train), \n",
    "                  'Test Accuracy':accuracy_score(yvalid, y_pred_test),\n",
    "                  'Best Accuracy': grid.best_score_,\n",
    "                  'Train Log Loss':log_loss(ytrain, y_pred_train), \n",
    "                  'Test Log Loss':log_loss(yvalid, y_pred_test),\n",
    "                  'Train ROC AUC': roc_auc_score(ytrain, y_train_proba.T[1]), \n",
    "                  'Test ROC AUC': roc_auc_score(yvalid, y_test_proba.T[1]),\n",
    "                  'Best Param Settings' :grid.best_params_}\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = {}\n",
    "accuracies = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss for RandomForest(TF-IDF) on Test TF-IDF Vectorizer : 1.439139004712993\n",
      "ROC AUC for RandomForest(TF-IDF) on Test TF-IDF Vectorizer : 0.9967013888888889\n",
      "Accuracy for RandomForest(TF-IDF) on Test TF-IDF Vectorizer : 0.9583333333333334\n",
      "Log Loss for RandomForest(TF-IDF) on Training TF-IDF Vectorizer : 1.0713587036762993\n",
      "ROC AUC for RandomForest(TF-IDF) on Training TF-IDF Vectorizer : 0.9978939471879287\n",
      "Accuracy for RandomForest(TF-IDF) on Training TF-IDF Vectorizer : 0.9689814814814814\n",
      "Best Score : 70.62\n",
      "Best Paramerter Settings :  {'randomforestclassifier__bootstrap': False, 'randomforestclassifier__max_depth': None, 'randomforestclassifier__min_samples_leaf': 1, 'randomforestclassifier__n_estimators': 100, 'tfidfvectorizer__ngram_range': (1, 2)}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fitting a simple Random Forest on TFIDF#########\n",
    "sqrt_features = int(np.sqrt(xtrain_tfv.shape[1]))\n",
    "params_rf = {'randomforestclassifier__n_estimators': [1, 8, 32, 64, 100, 200, 500],\n",
    "                'randomforestclassifier__min_samples_leaf': [0.1,0.2,0.3,0.4,0.5,1],\n",
    "                 'randomforestclassifier__max_depth' : [None],\n",
    "                'randomforestclassifier__bootstrap' : [False],\n",
    "                \"tfidfvectorizer__ngram_range\": [(1, 1), (1, 2), (1, 3), (2, 2)]}\n",
    "pipeline_rf = make_pipeline(TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english'), RandomForestClassifier())\n",
    "\n",
    "accuracies['RandomForest(TF-IDF)'] = grid_search_best_params('RandomForest(TF-IDF)',pipeline_rf, params_rf,xtrain1,ytrain1,xvalid,yvalid,x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "n_estimators_param_range = [1, 8, 32, 64, 100, 200, 500]\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Apply logistic regression model to training data\n",
    "lr = RandomForestClassifier(n_estimators=100, random_state=4242)\n",
    "\n",
    "# SEPAL Plot validation curve\n",
    "train_scores, test_scores = validation_curve(estimator=lr\n",
    "                                                            ,X=xtrain_tfv\n",
    "                                                            ,y=ytrain\n",
    "                                                            ,param_name='n_estimators'\n",
    "                                                            ,param_range=n_estimators_param_range\n",
    "                                                            )\n",
    "\n",
    "train_mean = np.mean(train_scores,axis=1)\n",
    "train_std = np.std(train_scores,axis=1)\n",
    "test_mean = np.mean(test_scores,axis=1)\n",
    "test_std = np.std(test_scores,axis=1)\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(n_estimators_param_range\n",
    "            ,train_mean\n",
    "            ,color='blue'\n",
    "            ,marker='o'\n",
    "            ,markersize=5\n",
    "            ,label='training accuracy')\n",
    "    \n",
    "plt.plot(n_estimators_param_range\n",
    "            ,test_mean\n",
    "            ,color='green'\n",
    "            ,marker='x'\n",
    "            ,markersize=5\n",
    "            ,label='test accuracy') \n",
    "    \n",
    "plt.xlabel('n_estimators_parameter')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim([0.5,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5, 1)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "min_samples_leaf_param_range = [0.1,0.2,0.3,0.4,0.5,1]\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Apply logistic regression model to training data\n",
    "lr = RandomForestClassifier(n_estimators=50, min_samples_leaf=1, random_state=4242)\n",
    "\n",
    "# SEPAL Plot validation curve\n",
    "train_scores, test_scores = validation_curve(estimator=lr\n",
    "                                                            ,X=xtrain_tfv\n",
    "                                                            ,y=ytrain\n",
    "                                                            ,param_name='min_samples_leafh'\n",
    "                                                            ,param_range=min_samples_leaf_param_range\n",
    "                                                            )\n",
    "\n",
    "train_mean = np.mean(train_scores,axis=1)\n",
    "train_std = np.std(train_scores,axis=1)\n",
    "test_mean = np.mean(test_scores,axis=1)\n",
    "test_std = np.std(test_scores,axis=1)\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(min_samples_leaf_param_range\n",
    "            ,train_mean\n",
    "            ,color='blue'\n",
    "            ,marker='o'\n",
    "            ,markersize=5\n",
    "            ,label='training accuracy')\n",
    "    \n",
    "plt.plot(min_samples_leaf_param_range\n",
    "            ,test_mean\n",
    "            ,color='green'\n",
    "            ,marker='x'\n",
    "            ,markersize=5\n",
    "            ,label='test accuracy') \n",
    "    \n",
    "plt.xlabel('min_samples_leaf_parameter')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim([0.5,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XkAYr-oU0gsq"
   },
   "source": [
    "### Fitting a Random Forest over count vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrt_features = int(np.sqrt(xtrain_tfv.shape[1]))\n",
    "params_rf = {'randomforestclassifier__n_estimators': [1, 2, 4, 8, 16, 32, 64, 100, 200, 300, 500, 800, 1000],\n",
    "                'randomforestclassifier__min_samples_leaf': np.linspace(0.1, 0.5, 5, endpoint=True),\n",
    "                'randomforestclassifier__max_depth' : [None, 3, 4, 5],\n",
    "                'randomforestclassifier__bootstrap' : [False],\n",
    "                \"countvectorizer__ngram_range\": [(1, 1), (1, 2), (2, 2), (1, 3)]}\n",
    "pipeline_rf = make_pipeline(CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            stop_words = 'english'), RandomForestClassifier())\n",
    "\n",
    "accuracies['RandomForest(CV)'] = grid_search_best_params('RandomForest(CV)',pipeline_rf, params_rf,xtrain1,ytrain1,xvalid,yvalid,x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "znS1QO_gJnCH"
   },
   "outputs": [],
   "source": [
    "mll_scorer = metrics.make_scorer(multiclass_logloss, greater_is_better=False, needs_proba=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cLG4PtCTtkur"
   },
   "source": [
    "## How out of vocab words are handled in Bag of Words\n",
    "*  As the name suggests, Its a bag of words ie only the words that are there in the bag are considered for prediction. \n",
    "*  Unknown words are ignored as they are not in the bag of words\n",
    "*  Unknown words are handled better in the section below where embeddings are used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jC4b_PVk1KCy"
   },
   "source": [
    "## Grid Search\n",
    "Grid search is the process of performing hyper parameter tuning in order to determine the optimal values for a given model. This is significant as the performance of the entire model is based on the hyper parameter values specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MnPBjsuSzS-6"
   },
   "source": [
    "## General intuition on k-fold cross validation\n",
    "\n",
    "*  The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into. As such, the procedure is often called k-fold cross-validation. When a specific value for k is chosen, it may be used in place of k in the reference to the model, such as k=10 becoming 10-fold cross-validation.\n",
    "\n",
    "*  Cross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data. That is, to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model.\n",
    "\n",
    "*  It is a popular method because it is simple to understand and because it generally results in a less biased or less optimistic estimate of the model skill than other methods, such as a simple train/test split.\n",
    "\n",
    "*  The general procedure is as follows:\n",
    "\n",
    "*  Shuffle the dataset randomly.\n",
    "*  Split the dataset into k groups\n",
    "*  For each unique group:\n",
    "*  Take the group as a hold out or test data set\n",
    "*  Take the remaining groups as a training data set\n",
    "*  Fit a model on the training set and evaluate it on the test set\n",
    "*  Retain the evaluation score and discard the model\n",
    "*  Summarize the skill of the model using the sample of model evaluation scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jl2xGyku3mGs"
   },
   "source": [
    "## Trying confusion matrix and plot accuracy/ 1-error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "KTShvTuUJnCn",
    "outputId": "55153240-c7fa-41e7-e216-8730f6c7c37b"
   },
   "outputs": [],
   "source": [
    "confusion_matrix=metrics.confusion_matrix(yvalid, y_pred_Linear)\n",
    "print(confusion_matrix)\n",
    "auc_roc=metrics.roc_auc_score(yvalid, y_pred_Linear)\n",
    "auc_roc\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(yvalid, y_pred_Linear)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in grids.items():\n",
    "    print(model_name + ' 5 Fold Log Loss(Error)')\n",
    "    df_lr = pd.DataFrame(model.cv_results_)\n",
    "    df_lr = df_lr[df_lr.params == model.best_params_]\n",
    "    errors = df_lr[['split0_test_neg_log_loss','split1_test_neg_log_loss','split2_test_neg_log_loss','split3_test_neg_log_loss','split4_test_neg_log_loss','mean_test_neg_log_loss']].values[0]\n",
    "    print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in grids.items():\n",
    "    print(model_name + '5 Fold ROC AUC')\n",
    "    df_lr = pd.DataFrame(model.cv_results_)\n",
    "    df_lr = df_lr[df_lr.params == model.best_params_]\n",
    "    roc_aucs = df_lr[['split0_test_roc_auc','split1_test_roc_auc','split2_test_roc_auc','split3_test_roc_auc','split4_test_roc_auc','mean_test_roc_auc']].values[0]\n",
    "    print(roc_aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in grids.items():\n",
    "    print(model_name+' 5 Fold Accuracies')\n",
    "    df_lr = pd.DataFrame(model.cv_results_)\n",
    "    df_lr = df_lr[df_lr.params == model.best_params_]\n",
    "    accuracies = df_lr[['split0_test_accuracy','split1_test_accuracy','split2_test_accuracy','split3_test_accuracy','split4_test_accuracy','mean_test_accuracy']].values[0]\n",
    "    print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We'll now use best parameters we found out above from grid search and plot model accuracy.\n",
    "plt.figure(figsize=(20,10))\n",
    "CV=5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(grids)))\n",
    "entries = []\n",
    "for model_name, model in grids.items():\n",
    "    #model_name = model.__class__.__name__\n",
    "    accuracies = df_lr = pd.DataFrame(model.cv_results_)\n",
    "    df_lr = df_lr[df_lr.params == model.best_params_]\n",
    "    #print(df_lr.columns)\n",
    "    accuracies = df_lr[['split0_test_accuracy','split1_test_accuracy','split2_test_accuracy','split3_test_accuracy','split4_test_accuracy','mean_test_accuracy']].values[0]\n",
    "    for fold_idx, accuracy in enumerate(accuracies):\n",
    "        entries.append((model_name, fold_idx, accuracy))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "\n",
    "sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n",
    "sns.stripplot(x='model_name', y='accuracy', data=cv_df, \n",
    "              size=8, jitter=True, edgecolor=\"gray\", linewidth=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "cv_df = pd.DataFrame(index=range(CV * len(grids)))\n",
    "entries = []\n",
    "for model_name, model in grids.items():\n",
    "    #model_name = model.__class__.__name__\n",
    "    df_lr = pd.DataFrame(model.cv_results_)\n",
    "    df_lr = df_lr[df_lr.params == model.best_params_]\n",
    "    #print(df_lr.columns)\n",
    "    roc_aucs = df_lr[['split0_test_roc_auc','split1_test_roc_auc','split2_test_roc_auc','split3_test_roc_auc','split4_test_roc_auc','mean_test_roc_auc']].values[0]\n",
    "    for fold_idx, roc_auc in enumerate(roc_aucs):\n",
    "        entries.append((model_name, fold_idx, roc_auc))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'roc_auc'])\n",
    "\n",
    "sns.boxplot(x='model_name', y='roc_auc', data=cv_df)\n",
    "sns.stripplot(x='model_name', y='roc_auc', data=cv_df, \n",
    "              size=8, jitter=True, edgecolor=\"gray\", linewidth=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rxy5gjleZoZC"
   },
   "source": [
    "Fitting the best result obtained from grid search in the random forest algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "ra3mfd4aJnF6",
    "outputId": "a26cd7c2-221c-4f88-aac2-53cdf8d310d4"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "classifier =rfc1\n",
    "from sklearn.model_selection import cross_val_score  \n",
    "all_accuracies = cross_val_score(estimator=classifier, X=xtrain_tfv, y=ytrain, cv=5)  \n",
    "\n",
    "print(all_accuracies)  \n",
    "print(all_accuracies.mean())\n",
    "print(all_accuracies.std())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating all models on the ALL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HOW TO DO IT? No polished figure, table OR explanation showing which features are most important to the best performing classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We'll now use best parameters we found out above from grid search and plot model accuracy.\n",
    "df = pd.DataFrame(accuracies)\n",
    "df = df.transpose()\n",
    "df = df.reset_index()\n",
    "df = df.rename(columns={'index':'model_name'})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x='model_name',y=['Train Accuracy', 'Test Accuracy','Best Accuracy'], kind='bar', figsize=(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x='model_name',y=['Train Log Loss', 'Test Log Loss'], kind='bar', figsize=(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x='model_name',y=['Train ROC AUC', 'Test ROC AUC'], kind='bar', figsize=(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a5AMBrt-5BNv"
   },
   "source": [
    "## Box plot\n",
    "A box plot shows the distribution of quantitative data in a way that facilitates comparisons between variables or across levels of a categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, grid in grids.items():\n",
    "    print(model_name)\n",
    "    print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_coefficients(classifier, feature_names, n_top_features=25):\n",
    "    # get coefficients with large absolute values \n",
    "    coef = classifier.coef_.ravel()\n",
    "    positive_coefficients = np.argsort(coef)[-n_top_features:]\n",
    "    negative_coefficients = np.argsort(coef)[:n_top_features]\n",
    "    interesting_coefficients = np.hstack([negative_coefficients, positive_coefficients])\n",
    "    # plot them\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    colors = [\"red\" if c < 0 else \"blue\" for c in coef[interesting_coefficients]]\n",
    "    plt.bar(np.arange(50), coef[interesting_coefficients], color=colors)\n",
    "    feature_names = np.array(feature_names)\n",
    "    plt.xticks(np.arange(1, 51), feature_names[interesting_coefficients], rotation=60, ha=\"right\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######Count vector Creation##############\n",
    "ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 1), stop_words = 'english')\n",
    "\n",
    "# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\n",
    "ctv.fit(list(xtrain1) + list(xvalid))\n",
    "xtrain_ctv =  ctv.transform(xtrain1) \n",
    "xvalid_ctv = ctv.transform(xvalid)\n",
    "lr = LogisticRegression(C=1, penalty='l2')\n",
    "lr.fit(xtrain_ctv,ytrain1)\n",
    "\n",
    "visualize_coefficients(lr, ctv.get_feature_names())\n",
    "plt.title('Logistic Regression with Count Vectorizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#{'randomforestclassifier__bootstrap': True, 'randomforestclassifier__max_depth': None, \n",
    "# 'randomforestclassifier__min_samples_leaf': 1, 'randomforestclassifier__n_estimators': 68, 'tfidfvectorizer__ngram_range': (1, 3)}\n",
    "######tfidf vector Creation##############\n",
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 1), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "\n",
    "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
    "tfv.fit(list(xtrain1) + list(xvalid))\n",
    "xtrain_tfv =  tfv.transform(xtrain1) \n",
    "xvalid_tfv = tfv.transform(xvalid)\n",
    "lr = LogisticRegression(C=1, penalty='l2')\n",
    "lr.fit(xtrain_tfv,ytrain1)\n",
    "\n",
    "visualize_coefficients(lr, tfv.get_feature_names())\n",
    "plt.title('Logistic Regression with TF-IDF Vectorizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We'll now use best parameters we found out above from grid search and plot model accuracy.\n",
    "plt.figure(figsize=(20,10))\n",
    "CV=5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(grids)))\n",
    "entries = []\n",
    "for model_name, model in grids.items():\n",
    "    #model_name = model.__class__.__name__\n",
    "    accuracies = df_lr = pd.DataFrame(model.cv_results_)\n",
    "    df_lr = df_lr[df_lr.params == model.best_params_]\n",
    "    #print(df_lr.columns)\n",
    "    accuracies = df_lr[['split0_test_accuracy','split1_test_accuracy','split2_test_accuracy','split3_test_accuracy','split4_test_accuracy','mean_test_accuracy']].values[0]\n",
    "    for fold_idx, accuracy in enumerate(accuracies):\n",
    "        entries.append((model_name, fold_idx, accuracy))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "\n",
    "sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n",
    "sns.stripplot(x='model_name', y='accuracy', data=cv_df, \n",
    "              size=8, jitter=True, edgecolor=\"gray\", linewidth=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We'll now use best parameters we found out above from grid search and plot model accuracy.\n",
    "plt.figure(figsize=(20,10))\n",
    "cv_df = pd.DataFrame(index=range(CV * len(grids)))\n",
    "entries = []\n",
    "for model_name, model in grids.items():\n",
    "    #model_name = model.__class__.__name__\n",
    "    df_lr = pd.DataFrame(model.cv_results_)\n",
    "    df_lr = df_lr[df_lr.params == model.best_params_]\n",
    "    #print(df_lr.columns)\n",
    "    losses = df_lr[['split0_test_neg_log_loss','split1_test_neg_log_loss','split2_test_neg_log_loss','split3_test_neg_log_loss','split4_test_neg_log_loss','mean_test_neg_log_loss']].values[0]\n",
    "    for fold_idx, loss in enumerate(losses):\n",
    "        entries.append((model_name, fold_idx, loss))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'log_loss'])\n",
    "\n",
    "sns.boxplot(x='model_name', y='log_loss', data=cv_df)\n",
    "sns.stripplot(x='model_name', y='log_loss', data=cv_df, \n",
    "              size=8, jitter=True, edgecolor=\"gray\", linewidth=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "cv_df = pd.DataFrame(index=range(CV * len(grids)))\n",
    "entries = []\n",
    "for model_name, model in grids.items():\n",
    "    #model_name = model.__class__.__name__\n",
    "    df_lr = pd.DataFrame(model.cv_results_)\n",
    "    df_lr = df_lr[df_lr.params == model.best_params_]\n",
    "    #print(df_lr.columns)\n",
    "    roc_aucs = df_lr[['split0_test_roc_auc','split1_test_roc_auc','split2_test_roc_auc','split3_test_roc_auc','split4_test_roc_auc','mean_test_roc_auc']].values[0]\n",
    "    for fold_idx, roc_auc in enumerate(roc_aucs):\n",
    "        entries.append((model_name, fold_idx, roc_auc))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'roc_auc'])\n",
    "\n",
    "sns.boxplot(x='model_name', y='roc_auc', data=cv_df)\n",
    "sns.stripplot(x='model_name', y='roc_auc', data=cv_df, \n",
    "              size=8, jitter=True, edgecolor=\"gray\", linewidth=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in grids.items():\n",
    "    print(model_name + ' 5 Fold Log Loss(Error)')\n",
    "    df_lr = pd.DataFrame(model.cv_results_)\n",
    "    df_lr = df_lr[df_lr.params == model.best_params_]\n",
    "    errors = df_lr[['split0_test_neg_log_loss','split1_test_neg_log_loss','split2_test_neg_log_loss','split3_test_neg_log_loss','split4_test_neg_log_loss','mean_test_neg_log_loss']].values[0]\n",
    "    print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in grids.items():\n",
    "    print(model_name + '5 Fold ROC AUC')\n",
    "    df_lr = pd.DataFrame(model.cv_results_)\n",
    "    df_lr = df_lr[df_lr.params == model.best_params_]\n",
    "    roc_aucs = df_lr[['split0_test_roc_auc','split1_test_roc_auc','split2_test_roc_auc','split3_test_roc_auc','split4_test_roc_auc','mean_test_roc_auc']].values[0]\n",
    "    print(roc_aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in grids.items():\n",
    "    print(model_name+' 5 Fold Error Rate')\n",
    "    df_lr = pd.DataFrame(model.cv_results_)\n",
    "    df_lr = df_lr[df_lr.params == model.best_params_]\n",
    "    accuracies = df_lr[['split0_test_accuracy','split1_test_accuracy','split2_test_accuracy','split3_test_accuracy','split4_test_accuracy','mean_test_accuracy']].values[0]\n",
    "    print(1-accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, grid in grids.items():\n",
    "    submission_predictions = grid.predict_proba(x_test)[:,1]\n",
    "    model_name = model_name.replace('(','_').replace(')','').replace('-','_')\n",
    "    np.savetxt(\"yproba_part1_%s.txt\"%model_name, submission_predictions,newline='\\r\\n')\n",
    "    #print(display.HTML('<a href=\"yproba_part1_%s.txt\" target=\"_blank\">Y_TEST_PROBA_PART1_%s</a>'%(model_name,model_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EAnHoru95wHm"
   },
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OS8mPVohZMcP"
   },
   "source": [
    "# Concatinating the predicted value and the review in one csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C8G98y4hJnGu"
   },
   "outputs": [],
   "source": [
    "x_train = pd.read_csv('x_train.csv')\n",
    "y_train = pd.read_csv('y_train.csv')\n",
    "df=pd.concat([x_train,y_train], axis=1)\n",
    "df['Sentiment'] = np.where(df['is_positive_sentiment']== 1, 'Positive', 'Negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "tljJLypZJnGy",
    "outputId": "2ed242e7-9b7f-4b9c-928e-110d398d00ed"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "6_zhaNk2JnG_",
    "outputId": "58eb2c3b-886c-49b0-9e91-3d43dbf68ba7"
   },
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "col = ['Sentiment', 'text']\n",
    "df = df[col]\n",
    "df = df[pd.notnull(df['text'])]\n",
    "df.columns = ['Sentiment', 'text']\n",
    "df['Sentiment_id'] = df['Sentiment'].factorize()[0]\n",
    "Sentiment_id_df = df[['Sentiment', 'Sentiment_id']].drop_duplicates().sort_values('Sentiment_id')\n",
    "Sentiment_to_id = dict(Sentiment_id_df.values)\n",
    "id_to_Sentiment = dict(Sentiment_id_df[['Sentiment_id', 'Sentiment']].values)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K_ojtN-3JnHN"
   },
   "outputs": [],
   "source": [
    "xtrain_ctv =  ctv.transform(x_train['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UMz4aMAkYH8-"
   },
   "source": [
    "# Error Analysis\n",
    "### printing out all the predictions that were proved to be incorrect by the test set and have a conf_mat deviation of more than 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a-9VkldrYhbw"
   },
   "source": [
    "printing out all the negatives makred positve and all positives marked negative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "BWhLqa35JnHi",
    "outputId": "5fa56b43-180a-4261-9415-ec793dab6910"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "for predicted in Sentiment_id_df.Sentiment_id:\n",
    "  for actual in Sentiment_id_df.Sentiment_id:\n",
    "    if predicted != actual and conf_mat[actual, predicted] >= 10:\n",
    "      print(\"'{}' predicted as '{}' : {} examples.\".format(id_to_Sentiment[actual], id_to_Sentiment[predicted], conf_mat[actual, predicted]))\n",
    "      display(df.loc[indices_test[(y_test == actual) & (y_pred == predicted)]][['Sentiment', 'text']])\n",
    "      print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HRGum0xO7psR"
   },
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0gRzDPfM7yro"
   },
   "source": [
    "# **Word** **embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bgMhMqwXIQ9E"
   },
   "source": [
    "# Sentiment analysis by creating word vectors\n",
    "Word Embedding is a representation of text where words that have the same meaning have a similar representation. In other words it represents words in a coordinate system where related words, based on a corpus of relationships, are placed closer together. In the deep learning frameworks such as TensorFlow, Keras, this part is usually handled by an embedding layer which stores a lookup table to map the words represented by numeric indexes to their dense vector representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WgfBIDCv_Ded"
   },
   "source": [
    "## Glove\n",
    "Glove is an unsupervised algorithm to learn and store these contexts between a large number of words. Glove comes as a pre trained model and we will emply it in our work below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup \n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection  import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "from textblob import TextBlob, Word, Blobber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import numpy\n",
    "from keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup \n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection  import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pd.options.display.max_columns = 30\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob, Word, Blobber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('/Users/user/Downloads/project2/data/data_reviews') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = pd.read_csv('x_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv('x_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>website_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>amazon</td>\n",
       "      <td>Oh and I forgot to also mention the weird colo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>amazon</td>\n",
       "      <td>THAT one didn't work either.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>amazon</td>\n",
       "      <td>Waste of 13 bucks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>amazon</td>\n",
       "      <td>Product is useless, since it does not have eno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>amazon</td>\n",
       "      <td>None of the three sizes they sent with the hea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  website_name                                               text\n",
       "0       amazon  Oh and I forgot to also mention the weird colo...\n",
       "1       amazon                       THAT one didn't work either.\n",
       "2       amazon                                 Waste of 13 bucks.\n",
       "3       amazon  Product is useless, since it does not have eno...\n",
       "4       amazon  None of the three sizes they sent with the hea..."
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.read_csv('y_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_positive_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_positive_sentiment\n",
       "0                      0\n",
       "1                      0\n",
       "2                      0\n",
       "3                      0\n",
       "4                      0"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "###LOWER CASE############\n",
    "x_train[\"text\"] = (x_train[\"text\"].apply(lambda x: \" \".join(x.lower() for x in x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing punctuation\n",
    "x_train[\"text\"] = x_train[\"text\"].str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "####lEMENTIZATIOO#############\n",
    "x_train[\"text\"]  = (x_train[\"text\"].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_words( raw_data ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    \n",
    "    # 1. Remove HTML\n",
    "    text = BeautifulSoup(raw_data).get_text() \n",
    "    \n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text) \n",
    "    \n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    \n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "     \n",
    "    # 5. Remove stop words\n",
    "    #meaningful_words = [w for w in words if not w in stops]   \n",
    "    meaningful_words = words\n",
    "    \n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400\n"
     ]
    }
   ],
   "source": [
    "# Get the number of text based on the dataframe column size\n",
    "num_text = x_train['text'].size\n",
    "print (num_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text 1000 of 2400\n",
      "\n",
      "text 2000 of 2400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_df_text = []\n",
    "for i in range( 0, num_text ):\n",
    "    # If the index is evenly divisible by 1000, print a message\n",
    "    if((i+1)%1000 == 0):\n",
    "        print (\"text %d of %d\\n\" % ( i+1, num_text ) )                                                                   \n",
    "    clean_df_text.append( text_to_words( x_train[\"text\"][i] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "###X_train_counts = count_vect.fit_transform(x_train['text'])\n",
    "X_train_counts = count_vect.fit_transform(clean_df_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abhor</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abound</th>\n",
       "      <th>about</th>\n",
       "      <th>above</th>\n",
       "      <th>abovepretty</th>\n",
       "      <th>abroad</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutel</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>absolutley</th>\n",
       "      <th>abstruse</th>\n",
       "      <th>abysmal</th>\n",
       "      <th>...</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>your</th>\n",
       "      <th>youre</th>\n",
       "      <th>yourself</th>\n",
       "      <th>youthful</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yucky</th>\n",
       "      <th>yukon</th>\n",
       "      <th>yum</th>\n",
       "      <th>yummy</th>\n",
       "      <th>yun</th>\n",
       "      <th>zero</th>\n",
       "      <th>zillion</th>\n",
       "      <th>zombie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2395</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2396</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2397</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2398</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2399</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2400 rows × 4231 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      abandoned  abhor  ability  able  abound  about  above  abovepretty  \\\n",
       "0             0      0        0     0       0      0      0            0   \n",
       "1             0      0        0     0       0      0      0            0   \n",
       "2             0      0        0     0       0      0      0            0   \n",
       "3             0      0        0     0       0      0      0            0   \n",
       "4             0      0        0     0       0      0      0            0   \n",
       "...         ...    ...      ...   ...     ...    ...    ...          ...   \n",
       "2395          0      0        0     0       0      0      0            0   \n",
       "2396          0      0        0     0       0      0      0            0   \n",
       "2397          0      0        0     0       0      0      0            0   \n",
       "2398          0      0        0     0       0      0      0            0   \n",
       "2399          0      0        0     0       0      0      0            0   \n",
       "\n",
       "      abroad  absolute  absolutel  absolutely  absolutley  abstruse  abysmal  \\\n",
       "0          0         0          0           0           0         0        0   \n",
       "1          0         0          0           0           0         0        0   \n",
       "2          0         0          0           0           0         0        0   \n",
       "3          0         0          0           0           0         0        0   \n",
       "4          0         0          0           0           0         0        0   \n",
       "...      ...       ...        ...         ...         ...       ...      ...   \n",
       "2395       0         0          0           0           0         0        0   \n",
       "2396       0         0          0           0           0         0        0   \n",
       "2397       0         0          0           0           0         0        0   \n",
       "2398       0         0          0           0           0         0        0   \n",
       "2399       0         0          0           0           0         0        0   \n",
       "\n",
       "      ...  young  younger  your  youre  yourself  youthful  youtube  yucky  \\\n",
       "0     ...      0        0     1      0         0         0        0      0   \n",
       "1     ...      0        0     0      0         0         0        0      0   \n",
       "2     ...      0        0     0      0         0         0        0      0   \n",
       "3     ...      0        0     0      0         0         0        0      0   \n",
       "4     ...      0        0     0      0         0         0        0      0   \n",
       "...   ...    ...      ...   ...    ...       ...       ...      ...    ...   \n",
       "2395  ...      0        0     0      0         0         0        0      0   \n",
       "2396  ...      0        0     0      0         0         0        0      0   \n",
       "2397  ...      0        0     0      0         0         0        0      0   \n",
       "2398  ...      0        0     0      0         0         0        0      0   \n",
       "2399  ...      0        0     0      0         0         0        0      0   \n",
       "\n",
       "      yukon  yum  yummy  yun  zero  zillion  zombie  \n",
       "0         0    0      0    0     0        0       0  \n",
       "1         0    0      0    0     0        0       0  \n",
       "2         0    0      0    0     0        0       0  \n",
       "3         0    0      0    0     0        0       0  \n",
       "4         0    0      0    0     0        0       0  \n",
       "...     ...  ...    ...  ...   ...      ...     ...  \n",
       "2395      0    0      0    0     0        0       0  \n",
       "2396      0    0      0    0     0        0       0  \n",
       "2397      0    0      0    0     0        0       0  \n",
       "2398      0    0      0    0     0        0       0  \n",
       "2399      0    0      0    0     0        0       0  \n",
       "\n",
       "[2400 rows x 4231 columns]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_train_counts.toarray(), columns=count_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abhor</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abound</th>\n",
       "      <th>about</th>\n",
       "      <th>above</th>\n",
       "      <th>abovepretty</th>\n",
       "      <th>abroad</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutel</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>absolutley</th>\n",
       "      <th>abstruse</th>\n",
       "      <th>abysmal</th>\n",
       "      <th>...</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>your</th>\n",
       "      <th>youre</th>\n",
       "      <th>yourself</th>\n",
       "      <th>youthful</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yucky</th>\n",
       "      <th>yukon</th>\n",
       "      <th>yum</th>\n",
       "      <th>yummy</th>\n",
       "      <th>yun</th>\n",
       "      <th>zero</th>\n",
       "      <th>zillion</th>\n",
       "      <th>zombie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.258199</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2395</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2396</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2397</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2398</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2399</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2400 rows × 4231 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      abandoned  abhor  ability  able  abound  about  above  abovepretty  \\\n",
       "0           0.0    0.0      0.0   0.0     0.0    0.0    0.0          0.0   \n",
       "1           0.0    0.0      0.0   0.0     0.0    0.0    0.0          0.0   \n",
       "2           0.0    0.0      0.0   0.0     0.0    0.0    0.0          0.0   \n",
       "3           0.0    0.0      0.0   0.0     0.0    0.0    0.0          0.0   \n",
       "4           0.0    0.0      0.0   0.0     0.0    0.0    0.0          0.0   \n",
       "...         ...    ...      ...   ...     ...    ...    ...          ...   \n",
       "2395        0.0    0.0      0.0   0.0     0.0    0.0    0.0          0.0   \n",
       "2396        0.0    0.0      0.0   0.0     0.0    0.0    0.0          0.0   \n",
       "2397        0.0    0.0      0.0   0.0     0.0    0.0    0.0          0.0   \n",
       "2398        0.0    0.0      0.0   0.0     0.0    0.0    0.0          0.0   \n",
       "2399        0.0    0.0      0.0   0.0     0.0    0.0    0.0          0.0   \n",
       "\n",
       "      abroad  absolute  absolutel  absolutely  absolutley  abstruse  abysmal  \\\n",
       "0        0.0       0.0        0.0         0.0         0.0       0.0      0.0   \n",
       "1        0.0       0.0        0.0         0.0         0.0       0.0      0.0   \n",
       "2        0.0       0.0        0.0         0.0         0.0       0.0      0.0   \n",
       "3        0.0       0.0        0.0         0.0         0.0       0.0      0.0   \n",
       "4        0.0       0.0        0.0         0.0         0.0       0.0      0.0   \n",
       "...      ...       ...        ...         ...         ...       ...      ...   \n",
       "2395     0.0       0.0        0.0         0.0         0.0       0.0      0.0   \n",
       "2396     0.0       0.0        0.0         0.0         0.0       0.0      0.0   \n",
       "2397     0.0       0.0        0.0         0.0         0.0       0.0      0.0   \n",
       "2398     0.0       0.0        0.0         0.0         0.0       0.0      0.0   \n",
       "2399     0.0       0.0        0.0         0.0         0.0       0.0      0.0   \n",
       "\n",
       "      ...  young  younger      your  youre  yourself  youthful  youtube  \\\n",
       "0     ...    0.0      0.0  0.258199    0.0       0.0       0.0      0.0   \n",
       "1     ...    0.0      0.0  0.000000    0.0       0.0       0.0      0.0   \n",
       "2     ...    0.0      0.0  0.000000    0.0       0.0       0.0      0.0   \n",
       "3     ...    0.0      0.0  0.000000    0.0       0.0       0.0      0.0   \n",
       "4     ...    0.0      0.0  0.000000    0.0       0.0       0.0      0.0   \n",
       "...   ...    ...      ...       ...    ...       ...       ...      ...   \n",
       "2395  ...    0.0      0.0  0.000000    0.0       0.0       0.0      0.0   \n",
       "2396  ...    0.0      0.0  0.000000    0.0       0.0       0.0      0.0   \n",
       "2397  ...    0.0      0.0  0.000000    0.0       0.0       0.0      0.0   \n",
       "2398  ...    0.0      0.0  0.000000    0.0       0.0       0.0      0.0   \n",
       "2399  ...    0.0      0.0  0.000000    0.0       0.0       0.0      0.0   \n",
       "\n",
       "      yucky  yukon  yum  yummy  yun  zero  zillion  zombie  \n",
       "0       0.0    0.0  0.0    0.0  0.0   0.0      0.0     0.0  \n",
       "1       0.0    0.0  0.0    0.0  0.0   0.0      0.0     0.0  \n",
       "2       0.0    0.0  0.0    0.0  0.0   0.0      0.0     0.0  \n",
       "3       0.0    0.0  0.0    0.0  0.0   0.0      0.0     0.0  \n",
       "4       0.0    0.0  0.0    0.0  0.0   0.0      0.0     0.0  \n",
       "...     ...    ...  ...    ...  ...   ...      ...     ...  \n",
       "2395    0.0    0.0  0.0    0.0  0.0   0.0      0.0     0.0  \n",
       "2396    0.0    0.0  0.0    0.0  0.0   0.0      0.0     0.0  \n",
       "2397    0.0    0.0  0.0    0.0  0.0   0.0      0.0     0.0  \n",
       "2398    0.0    0.0  0.0    0.0  0.0   0.0      0.0     0.0  \n",
       "2399    0.0    0.0  0.0    0.0  0.0   0.0      0.0     0.0  \n",
       "\n",
       "[2400 rows x 4231 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same usage as CountVectorizer, as special setting here: use_idf=False. Setting norm = 'l1' makes it easier to see t\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=False)\n",
    "X = tfidf_vectorizer.fit_transform(clean_df_text)\n",
    "df = pd.DataFrame(X.toarray(), columns=tfidf_vectorizer.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abhor</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abound</th>\n",
       "      <th>about</th>\n",
       "      <th>above</th>\n",
       "      <th>abovepretty</th>\n",
       "      <th>abroad</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutel</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>absolutley</th>\n",
       "      <th>abstruse</th>\n",
       "      <th>abysmal</th>\n",
       "      <th>...</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>your</th>\n",
       "      <th>youre</th>\n",
       "      <th>yourself</th>\n",
       "      <th>youthful</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yucky</th>\n",
       "      <th>yukon</th>\n",
       "      <th>yum</th>\n",
       "      <th>yummy</th>\n",
       "      <th>yun</th>\n",
       "      <th>zero</th>\n",
       "      <th>zillion</th>\n",
       "      <th>zombie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232949</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2395</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2396</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2397</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2398</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2399</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2400 rows × 4231 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      abandoned  abhor  ability  able  abound  about  above  abovepretty  \\\n",
       "0           0.0    0.0      0.0   0.0     0.0    0.0    0.0          0.0   \n",
       "1           0.0    0.0      0.0   0.0     0.0    0.0    0.0          0.0   \n",
       "2           0.0    0.0      0.0   0.0     0.0    0.0    0.0          0.0   \n",
       "3           0.0    0.0      0.0   0.0     0.0    0.0    0.0          0.0   \n",
       "4           0.0    0.0      0.0   0.0     0.0    0.0    0.0          0.0   \n",
       "...         ...    ...      ...   ...     ...    ...    ...          ...   \n",
       "2395        0.0    0.0      0.0   0.0     0.0    0.0    0.0          0.0   \n",
       "2396        0.0    0.0      0.0   0.0     0.0    0.0    0.0          0.0   \n",
       "2397        0.0    0.0      0.0   0.0     0.0    0.0    0.0          0.0   \n",
       "2398        0.0    0.0      0.0   0.0     0.0    0.0    0.0          0.0   \n",
       "2399        0.0    0.0      0.0   0.0     0.0    0.0    0.0          0.0   \n",
       "\n",
       "      abroad  absolute  absolutel  absolutely  absolutley  abstruse  abysmal  \\\n",
       "0        0.0       0.0        0.0         0.0         0.0       0.0      0.0   \n",
       "1        0.0       0.0        0.0         0.0         0.0       0.0      0.0   \n",
       "2        0.0       0.0        0.0         0.0         0.0       0.0      0.0   \n",
       "3        0.0       0.0        0.0         0.0         0.0       0.0      0.0   \n",
       "4        0.0       0.0        0.0         0.0         0.0       0.0      0.0   \n",
       "...      ...       ...        ...         ...         ...       ...      ...   \n",
       "2395     0.0       0.0        0.0         0.0         0.0       0.0      0.0   \n",
       "2396     0.0       0.0        0.0         0.0         0.0       0.0      0.0   \n",
       "2397     0.0       0.0        0.0         0.0         0.0       0.0      0.0   \n",
       "2398     0.0       0.0        0.0         0.0         0.0       0.0      0.0   \n",
       "2399     0.0       0.0        0.0         0.0         0.0       0.0      0.0   \n",
       "\n",
       "      ...  young  younger      your  youre  yourself  youthful  youtube  \\\n",
       "0     ...    0.0      0.0  0.232949    0.0       0.0       0.0      0.0   \n",
       "1     ...    0.0      0.0  0.000000    0.0       0.0       0.0      0.0   \n",
       "2     ...    0.0      0.0  0.000000    0.0       0.0       0.0      0.0   \n",
       "3     ...    0.0      0.0  0.000000    0.0       0.0       0.0      0.0   \n",
       "4     ...    0.0      0.0  0.000000    0.0       0.0       0.0      0.0   \n",
       "...   ...    ...      ...       ...    ...       ...       ...      ...   \n",
       "2395  ...    0.0      0.0  0.000000    0.0       0.0       0.0      0.0   \n",
       "2396  ...    0.0      0.0  0.000000    0.0       0.0       0.0      0.0   \n",
       "2397  ...    0.0      0.0  0.000000    0.0       0.0       0.0      0.0   \n",
       "2398  ...    0.0      0.0  0.000000    0.0       0.0       0.0      0.0   \n",
       "2399  ...    0.0      0.0  0.000000    0.0       0.0       0.0      0.0   \n",
       "\n",
       "      yucky  yukon  yum  yummy  yun  zero  zillion  zombie  \n",
       "0       0.0    0.0  0.0    0.0  0.0   0.0      0.0     0.0  \n",
       "1       0.0    0.0  0.0    0.0  0.0   0.0      0.0     0.0  \n",
       "2       0.0    0.0  0.0    0.0  0.0   0.0      0.0     0.0  \n",
       "3       0.0    0.0  0.0    0.0  0.0   0.0      0.0     0.0  \n",
       "4       0.0    0.0  0.0    0.0  0.0   0.0      0.0     0.0  \n",
       "...     ...    ...  ...    ...  ...   ...      ...     ...  \n",
       "2395    0.0    0.0  0.0    0.0  0.0   0.0      0.0     0.0  \n",
       "2396    0.0    0.0  0.0    0.0  0.0   0.0      0.0     0.0  \n",
       "2397    0.0    0.0  0.0    0.0  0.0   0.0      0.0     0.0  \n",
       "2398    0.0    0.0  0.0    0.0  0.0   0.0      0.0     0.0  \n",
       "2399    0.0    0.0  0.0    0.0  0.0   0.0      0.0     0.0  \n",
       "\n",
       "[2400 rows x 4231 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "X = idf_vectorizer.fit_transform(clean_df_text)\n",
    "idf_df = pd.DataFrame(X.toarray(), columns=idf_vectorizer.get_feature_names())\n",
    "idf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "###LOWER CASE############\n",
    "x_train[\"text\"] = (x_train[\"text\"].apply(lambda x: \" \".join(x.lower() for x in x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing punctuation\n",
    "x_train[\"text\"] = x_train[\"text\"].str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "####lEMENTIZATIOO#############\n",
    "x_train[\"text\"]  = (x_train[\"text\"].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sentence, X_test_sentence, y_train_sentence, y_test_sentence = train_test_split( x_train['text'],y_train['is_positive_sentiment'],test_size=0.1, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "2506it [00:00, 25052.80it/s]\u001b[A\n",
      "4604it [00:00, 23673.49it/s]\u001b[A\n",
      "5760it [00:00, 14716.08it/s]\u001b[A\n",
      "6925it [00:00, 13638.28it/s]\u001b[A\n",
      "8875it [00:00, 14990.03it/s]\u001b[A\n",
      "10216it [00:00, 12827.44it/s]\u001b[A\n",
      "11674it [00:00, 13307.27it/s]\u001b[A\n",
      "12970it [00:00, 12520.13it/s]\u001b[A\n",
      "14268it [00:01, 12652.18it/s]\u001b[A\n",
      "15530it [00:01, 12590.75it/s]\u001b[A\n",
      "17117it [00:01, 13330.55it/s]\u001b[A\n",
      "18457it [00:01, 12098.99it/s]\u001b[A\n",
      "19694it [00:01, 11283.54it/s]\u001b[A\n",
      "21883it [00:01, 13201.93it/s]\u001b[A\n",
      "23351it [00:01, 13025.47it/s]\u001b[A\n",
      "24757it [00:01, 12192.53it/s]\u001b[A\n",
      "26116it [00:01, 12579.97it/s]\u001b[A\n",
      "27517it [00:02, 12947.57it/s]\u001b[A\n",
      "29479it [00:02, 14405.76it/s]\u001b[A\n",
      "31004it [00:02, 14227.73it/s]\u001b[A\n",
      "33188it [00:02, 15888.02it/s]\u001b[A\n",
      "34881it [00:02, 15031.57it/s]\u001b[A\n",
      "36467it [00:02, 13384.54it/s]\u001b[A\n",
      "38808it [00:02, 15357.18it/s]\u001b[A\n",
      "40511it [00:02, 15319.79it/s]\u001b[A\n",
      "42160it [00:02, 14929.07it/s]\u001b[A\n",
      "43737it [00:03, 15033.61it/s]\u001b[A\n",
      "45300it [00:03, 13697.75it/s]\u001b[A\n",
      "46735it [00:03, 13310.18it/s]\u001b[A\n",
      "48114it [00:03, 13381.67it/s]\u001b[A\n",
      "50097it [00:03, 14827.60it/s]\u001b[A\n",
      "52188it [00:03, 16245.09it/s]\u001b[A\n",
      "53904it [00:03, 15987.06it/s]\u001b[A\n",
      "55756it [00:03, 16670.77it/s]\u001b[A\n",
      "57982it [00:03, 17914.29it/s]\u001b[A\n",
      "59837it [00:04, 17396.23it/s]\u001b[A\n",
      "61625it [00:04, 17528.42it/s]\u001b[A\n",
      "63412it [00:04, 17228.59it/s]\u001b[A\n",
      "65819it [00:04, 18834.33it/s]\u001b[A\n",
      "67765it [00:04, 18753.90it/s]\u001b[A\n",
      "69810it [00:04, 19231.79it/s]\u001b[A\n",
      "71767it [00:04, 18517.44it/s]\u001b[A\n",
      "73648it [00:04, 15188.19it/s]\u001b[A\n",
      "75286it [00:04, 13089.38it/s]\u001b[A\n",
      "76730it [00:05, 11426.82it/s]\u001b[A\n",
      "78005it [00:05, 11090.33it/s]\u001b[A\n",
      "79209it [00:05, 10546.23it/s]\u001b[A\n",
      "80343it [00:05, 10772.20it/s]\u001b[A\n",
      "81767it [00:05, 11616.35it/s]\u001b[A\n",
      "83346it [00:05, 12562.62it/s]\u001b[A\n",
      "84663it [00:05, 12350.42it/s]\u001b[A\n",
      "85942it [00:05, 10967.34it/s]\u001b[A\n",
      "87364it [00:06, 11737.79it/s]\u001b[A\n",
      "88595it [00:06, 11363.99it/s]\u001b[A\n",
      "90688it [00:06, 13149.33it/s]\u001b[A\n",
      "92136it [00:06, 13012.79it/s]\u001b[A\n",
      "94347it [00:06, 14820.95it/s]\u001b[A\n",
      "96066it [00:06, 15458.90it/s]\u001b[A\n",
      "97763it [00:06, 15883.06it/s]\u001b[A\n",
      "99454it [00:06, 16177.07it/s]\u001b[A\n",
      "101431it [00:06, 17109.30it/s]\u001b[A\n",
      "103198it [00:07, 16600.38it/s]\u001b[A\n",
      "104901it [00:07, 16028.62it/s]\u001b[A\n",
      "106538it [00:07, 15830.61it/s]\u001b[A\n",
      "108146it [00:07, 12643.71it/s]\u001b[A\n",
      "109530it [00:07, 12525.23it/s]\u001b[A\n",
      "111444it [00:07, 13971.15it/s]\u001b[A\n",
      "113217it [00:07, 14918.01it/s]\u001b[A\n",
      "114805it [00:07, 13701.07it/s]\u001b[A\n",
      "116262it [00:07, 13469.95it/s]\u001b[A\n",
      "117671it [00:08, 13631.43it/s]\u001b[A\n",
      "119135it [00:08, 13918.62it/s]\u001b[A\n",
      "120559it [00:08, 13914.19it/s]\u001b[A\n",
      "121973it [00:08, 13976.32it/s]\u001b[A\n",
      "123433it [00:08, 14121.03it/s]\u001b[A\n",
      "124857it [00:08, 12618.61it/s]\u001b[A\n",
      "126157it [00:08, 12300.80it/s]\u001b[A\n",
      "128840it [00:08, 14686.56it/s]\u001b[A\n",
      "130526it [00:08, 14588.73it/s]\u001b[A\n",
      "132137it [00:09, 14748.35it/s]\u001b[A\n",
      "133719it [00:09, 14667.62it/s]\u001b[A\n",
      "136379it [00:09, 16947.92it/s]\u001b[A\n",
      "138257it [00:09, 17042.34it/s]\u001b[A\n",
      "140089it [00:09, 14237.80it/s]\u001b[A\n",
      "142481it [00:09, 16205.59it/s]\u001b[A\n",
      "144313it [00:09, 16099.31it/s]\u001b[A\n",
      "146071it [00:09, 15428.75it/s]\u001b[A\n",
      "147724it [00:10, 13340.72it/s]\u001b[A\n",
      "149185it [00:10, 13249.27it/s]\u001b[A\n",
      "150599it [00:10, 12398.68it/s]\u001b[A\n",
      "151913it [00:10, 11380.24it/s]\u001b[A\n",
      "153506it [00:10, 12446.19it/s]\u001b[A\n",
      "155352it [00:10, 13794.01it/s]\u001b[A\n",
      "156831it [00:10, 12702.36it/s]\u001b[A\n",
      "158188it [00:10, 12551.45it/s]\u001b[A\n",
      "159505it [00:11, 11478.85it/s]\u001b[A\n",
      "160767it [00:11, 11772.34it/s]\u001b[A\n",
      "163155it [00:11, 13883.81it/s]\u001b[A\n",
      "164997it [00:11, 14990.69it/s]\u001b[A\n",
      "166944it [00:11, 16101.91it/s]\u001b[A\n",
      "168685it [00:11, 15695.03it/s]\u001b[A\n",
      "170349it [00:11, 15649.54it/s]\u001b[A\n",
      "171980it [00:11, 13128.76it/s]\u001b[A\n",
      "173408it [00:11, 12525.33it/s]\u001b[A\n",
      "174833it [00:12, 12996.46it/s]\u001b[A\n",
      "176496it [00:12, 13907.10it/s]\u001b[A\n",
      "178695it [00:12, 15629.89it/s]\u001b[A\n",
      "181504it [00:12, 18028.39it/s]\u001b[A\n",
      "184930it [00:12, 21014.60it/s]\u001b[A\n",
      "187663it [00:12, 22579.42it/s]\u001b[A\n",
      "190995it [00:12, 24995.41it/s]\u001b[A\n",
      "193916it [00:12, 26125.76it/s]\u001b[A\n",
      "197209it [00:12, 27851.27it/s]\u001b[A\n",
      "200167it [00:13, 24768.14it/s]\u001b[A\n",
      "202828it [00:13, 23442.85it/s]\u001b[A\n",
      "205316it [00:13, 23428.05it/s]\u001b[A\n",
      "207760it [00:13, 23465.78it/s]\u001b[A\n",
      "210177it [00:13, 23118.34it/s]\u001b[A\n",
      "212540it [00:13, 22424.25it/s]\u001b[A\n",
      "214823it [00:13, 21208.67it/s]\u001b[A\n",
      "217622it [00:13, 22870.50it/s]\u001b[A\n",
      "220002it [00:13, 23140.08it/s]\u001b[A\n",
      "222364it [00:13, 22570.39it/s]\u001b[A\n",
      "224658it [00:14, 22574.70it/s]\u001b[A\n",
      "226941it [00:14, 21148.24it/s]\u001b[A\n",
      "229092it [00:14, 20387.52it/s]\u001b[A\n",
      "231162it [00:14, 16851.53it/s]\u001b[A\n",
      "233535it [00:14, 18452.88it/s]\u001b[A\n",
      "235795it [00:14, 19526.22it/s]\u001b[A\n",
      "237857it [00:14, 16300.66it/s]\u001b[A\n",
      "239654it [00:14, 16607.58it/s]\u001b[A\n",
      "241433it [00:15, 14319.59it/s]\u001b[A\n",
      "243004it [00:15, 14235.38it/s]\u001b[A\n",
      "244845it [00:15, 15273.81it/s]\u001b[A\n",
      "246854it [00:15, 16150.14it/s]\u001b[A\n",
      "248545it [00:15, 13764.19it/s]\u001b[A\n",
      "250854it [00:15, 15661.81it/s]\u001b[A\n",
      "253777it [00:15, 18195.02it/s]\u001b[A\n",
      "255870it [00:16, 14374.33it/s]\u001b[A\n",
      "257624it [00:16, 12212.98it/s]\u001b[A\n",
      "259284it [00:16, 13263.40it/s]\u001b[A\n",
      "260832it [00:16, 10512.48it/s]\u001b[A\n",
      "262128it [00:16, 10804.97it/s]\u001b[A\n",
      "264172it [00:16, 12583.69it/s]\u001b[A\n",
      "265663it [00:16, 10096.74it/s]\u001b[A\n",
      "267814it [00:17, 12008.18it/s]\u001b[A\n",
      "270038it [00:17, 13930.46it/s]\u001b[A\n",
      "271770it [00:17, 11730.94it/s]\u001b[A\n",
      "273240it [00:17, 12209.91it/s]\u001b[A\n",
      "275830it [00:17, 14510.90it/s]\u001b[A\n",
      "277992it [00:17, 16098.79it/s]\u001b[A\n",
      "279887it [00:17, 15200.33it/s]\u001b[A\n",
      "281617it [00:17, 14915.92it/s]\u001b[A\n",
      "283257it [00:18, 15035.37it/s]\u001b[A\n",
      "285330it [00:18, 16385.36it/s]\u001b[A\n",
      "287078it [00:18, 16601.73it/s]\u001b[A\n",
      "288815it [00:18, 15581.08it/s]\u001b[A\n",
      "291556it [00:18, 17897.64it/s]\u001b[A\n",
      "293516it [00:18, 17214.91it/s]\u001b[A\n",
      "295362it [00:18, 15960.20it/s]\u001b[A\n",
      "297514it [00:18, 17298.52it/s]\u001b[A\n",
      "299570it [00:18, 18162.07it/s]\u001b[A\n",
      "301472it [00:19, 15606.64it/s]\u001b[A\n",
      "303156it [00:19, 10174.44it/s]\u001b[A\n",
      "304932it [00:19, 11669.66it/s]\u001b[A\n",
      "306739it [00:19, 13055.37it/s]\u001b[A\n",
      "308515it [00:19, 14182.35it/s]\u001b[A\n",
      "310151it [00:19, 13897.15it/s]\u001b[A\n",
      "311694it [00:19, 13833.30it/s]\u001b[A\n",
      "314928it [00:20, 16699.53it/s]\u001b[A\n",
      "317134it [00:20, 18012.49it/s]\u001b[A\n",
      "319206it [00:20, 17549.90it/s]\u001b[A\n",
      "321153it [00:20, 11633.25it/s]\u001b[A\n",
      "322923it [00:20, 12965.98it/s]\u001b[A\n",
      "324545it [00:20, 12100.21it/s]\u001b[A\n",
      "326902it [00:20, 14168.16it/s]\u001b[A\n",
      "329241it [00:21, 16068.76it/s]\u001b[A\n",
      "331148it [00:21, 16844.34it/s]\u001b[A\n",
      "333053it [00:21, 15890.02it/s]\u001b[A\n",
      "335264it [00:21, 17354.11it/s]\u001b[A\n",
      "337205it [00:21, 17920.81it/s]\u001b[A\n",
      "339111it [00:21, 16994.33it/s]\u001b[A\n",
      "340900it [00:21, 15065.88it/s]\u001b[A\n",
      "342510it [00:21, 15010.87it/s]\u001b[A\n",
      "344084it [00:21, 14433.58it/s]\u001b[A\n",
      "345722it [00:22, 14966.85it/s]\u001b[A\n",
      "347263it [00:22, 12934.02it/s]\u001b[A\n",
      "348636it [00:22, 11317.88it/s]\u001b[A\n",
      "349860it [00:22, 6479.67it/s] \u001b[A\n",
      "350815it [00:22, 6884.01it/s]\u001b[A\n",
      "351939it [00:22, 7789.58it/s]\u001b[A\n",
      "353786it [00:23, 9424.40it/s]\u001b[A\n",
      "355022it [00:23, 8423.13it/s]\u001b[A\n",
      "356460it [00:23, 9608.84it/s]\u001b[A\n",
      "359261it [00:23, 11967.26it/s]\u001b[A\n",
      "361572it [00:23, 13989.21it/s]\u001b[A\n",
      "363817it [00:23, 15771.57it/s]\u001b[A\n",
      "366262it [00:23, 17651.06it/s]\u001b[A\n",
      "369934it [00:23, 20908.03it/s]\u001b[A\n",
      "372632it [00:23, 22417.63it/s]\u001b[A\n",
      "375249it [00:24, 14289.16it/s]\u001b[A\n",
      "377751it [00:24, 16377.85it/s]\u001b[A\n",
      "379942it [00:24, 17236.29it/s]\u001b[A\n",
      "382405it [00:24, 18921.27it/s]\u001b[A\n",
      "384626it [00:24, 13247.84it/s]\u001b[A\n",
      "386415it [00:25, 13216.01it/s]\u001b[A\n",
      "389375it [00:25, 15847.38it/s]\u001b[A\n",
      "392610it [00:25, 18709.85it/s]\u001b[A\n",
      "395009it [00:25, 19228.49it/s]\u001b[A\n",
      "397579it [00:25, 20799.67it/s]\u001b[A\n",
      "400000it [00:25, 15621.68it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "########## Using Embedded Word Vector Glove#######\n",
    "embeddings_index = {}\n",
    "f = open('/Users/user/Downloads/project2/data/pretrained_word_embeddings/glove.6B.50d.txt',encoding=\"utf8\")\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(s,size):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return list(np.zeros(size))\n",
    "    return list(v / np.sqrt((v ** 2).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_glove = np.array([sent2vec(x,50) for x in X_train_sentence])\n",
    "xvalid_glove = np.array([sent2vec(x,50) for x in X_test_sentence])\n",
    "x_train_all = np.array([sent2vec(x,50) for x in x_train['text']])\n",
    "x_test_all = np.array([sent2vec(x,50) for x in x_test['text']])\n",
    "y_train_all = y_train['is_positive_sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mAvKmAPeAbG2"
   },
   "source": [
    "### Function to convert sentence to vector\n",
    "The input sentence in this function is, \n",
    "*  Converted to lower case\n",
    "*  Split into tokens\n",
    "*  Stripped off stop words\n",
    "*  All characters other than alphabets have been ommited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    :param actual: Array containing the actual target classes\n",
    "    :param predicted: Matrix with class predictions, one probability per class\n",
    "    Log loss, aka logistic loss or cross-entropy loss.\n",
    "    This is the loss function used in (multinomial) logistic regression and \n",
    "    extensions of it such as neural networks, defined as the negative log-likelihood of the true labels \n",
    "    given a probabilistic classifier’s predictions. The log loss is only defined for two or more labels.\n",
    "    Accuracy is the count of predictions where your predicted value equals the actual value. \n",
    "    Accuracy is not always a good indicator because of its yes or no nature.\n",
    "    Log Loss takes into account the uncertainty of your prediction based on how much it varies from the actual label. \n",
    "    This gives us a more nuanced view into the performance of our model.\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nlHDnKE2A1po"
   },
   "source": [
    "### create sentence vectors using the above function for training and validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YvX75S5jKdXD"
   },
   "source": [
    "### Word embeddings are computed differently. Each word is positioned into a multi-dimensional space. The number of dimensions in this space is chosen by the data scientist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "EZJ4zbXgkP8V",
    "outputId": "e5de2cb7-f89f-46ad-97b5-c9f73912a457"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m7bA-hSDBkEO"
   },
   "source": [
    "## Logistic Regression\n",
    "*  By default sklearn employs a penalty of \"l2\" when we call for logistic regression.\n",
    "*  Logistic regression is the right algorithm to start with classification algorithms. Eventhough, the name ‘Regression’ comes up, it is not a regression model, but a classification model. It uses a logistic function to frame binary output model. The output of the logistic regression will be a probability (0≤x≤1), and can be used to predict the binary 0 or 1 as the output ( if x<0.5, output= 0, else output=1).\n",
    "*  Cross entropy is used as the loss function in Logistic regression\n",
    "*  θ parameters explains the direction and intensity of significance\n",
    "independent variables over the dependent variable.\n",
    "*  Can be used for multiclass classifications also.\n",
    "*  Loss function is always convex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression Word2Vec features only\n",
    "#lr = LogisticRegression()\n",
    "#lr.fit(xtrain_glove, ytrain)\n",
    "#predictions = lr.predict(xvalid)\n",
    "#Trainpredictions= lr.predict(xtrain)\n",
    "\n",
    "#print(\"Accuracy Training score: \", accuracy_score(ytrain, Trainpredictions))\n",
    "#print(\"Accuracy  testing score: \", accuracy_score(yvalid, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = {}\n",
    "accuracies = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a simple Logistic Regression on TFIDF#########\n",
    "def grid_search_best_params(model_name, pipeline, params, xtrain, ytrain, xvalid, yvalid, X, Y):\n",
    "    grid = GridSearchCV(pipeline, param_grid=params, cv=5, n_jobs=-1,refit='accuracy',scoring=['accuracy','neg_log_loss','roc_auc'])\n",
    "    grid.fit(X, Y)\n",
    "    predictions = grid.predict_proba(xvalid)\n",
    "    grids[model_name] = copy.deepcopy(grid)\n",
    "    #print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n",
    "    y_pred_test = grid.predict(xvalid)\n",
    "    y_test_proba = grid.predict_proba(xvalid)\n",
    "    print(\"Log Loss for %s on Test with Embedding: %s\"%(model_name,log_loss(yvalid, y_pred_test)))\n",
    "    print(\"ROC AUC for %s on Test with Embedding: %s\"%(model_name,roc_auc_score(yvalid, y_test_proba.T[1])))\n",
    "    print(\"Accuracy for %s on Test with Embedding: %s\"%(model_name,accuracy_score(yvalid, y_pred_test)))\n",
    "    y_pred_train=grid.predict(xtrain)\n",
    "    y_train_proba = grid.predict_proba(xtrain)\n",
    "    print(\"Log Loss for %s on Training with Embedding: %s\"%(model_name,log_loss(ytrain, y_pred_train)))\n",
    "    print(\"ROC AUC for %s on Training with Embedding: %s\"%(model_name,roc_auc_score(ytrain, y_train_proba.T[1])))\n",
    "    print(\"Accuracy for %s on Training with Embedding: %s\"%(model_name, accuracy_score(ytrain,y_pred_train)))\n",
    "    print('Best Score : %.2f'%(grid.best_score_*100))\n",
    "    print('Best Paramerter Settings : ',grid.best_params_)\n",
    "    #print(\"ROC Area Under Curve in % : \",cross_val_score(pipeline_lr, X, Y, scoring=\"roc_auc\", cv=5)*100)\n",
    "    conf_mat = confusion_matrix(yvalid, y_pred_test)\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    sns.heatmap(conf_mat, annot=True, fmt='d')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show()\n",
    "    accuracies = {'Train Accuracy':accuracy_score(ytrain,y_pred_train), \n",
    "                  'Test Accuracy':accuracy_score(yvalid, y_pred_test),\n",
    "                  'Best Accuracy': grid.best_score_,\n",
    "                  'Train Log Loss':log_loss(ytrain, y_pred_train), \n",
    "                  'Test Log Loss':log_loss(yvalid, y_pred_test),\n",
    "                  'Train ROC AUC': roc_auc_score(ytrain, y_train_proba.T[1]), \n",
    "                  'Test ROC AUC': roc_auc_score(yvalid, y_test_proba.T[1]),\n",
    "                  'Best Param Settings' :grid.best_params_}\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple ones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss for LogisticRegression_Embedding on Test with Embedding: 7.9152395385017655\n",
      "ROC AUC for LogisticRegression_Embedding on Test with Embedding: 0.8457250365828165\n",
      "Accuracy for LogisticRegression_Embedding on Test with Embedding: 0.7708333333333334\n",
      "Log Loss for LogisticRegression_Embedding on Training with Embedding: 7.611411790444783\n",
      "ROC AUC for LogisticRegression_Embedding on Training with Embedding: 0.8542076956250733\n",
      "Accuracy for LogisticRegression_Embedding on Training with Embedding: 0.7796296296296297\n",
      "Best Score : 76.92\n",
      "Best Paramerter Settings :  {}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "params_lr = {\n",
    "          }\n",
    "lr = make_pipeline(LogisticRegression())\n",
    "\n",
    "accuracies['LogisticRegression(Embedding)'] = grid_search_best_params('LogisticRegression_Embedding', lr, params_lr,xtrain_glove,y_train_sentence,xvalid_glove,y_test_sentence,x_train_all,y_train_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss for LogisticRegression_Embedding on Test with Embedding: 7.7713279701896365\n",
      "ROC AUC for LogisticRegression_Embedding on Test with Embedding: 0.8469793045780781\n",
      "Accuracy for LogisticRegression_Embedding on Test with Embedding: 0.775\n",
      "Log Loss for LogisticRegression_Embedding on Training with Embedding: 7.419527478257974\n",
      "ROC AUC for LogisticRegression_Embedding on Training with Embedding: 0.8574768658834263\n",
      "Accuracy for LogisticRegression_Embedding on Training with Embedding: 0.7851851851851852\n",
      "Best Score : 77.12\n",
      "Best Paramerter Settings :  {'logisticregression__C': 10, 'logisticregression__penalty': 'l2'}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjYAAAJSCAYAAADQ0495AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAdCUlEQVR4nO3de5RlVX0n8O+Pbp4iaUReghofcMWwFEVZyWThREyMOoqi0SUrGh8kHSMYNdFoTMbojGMwmpi3k1ZUMhKi0WHp4MTIEMBolJd0CARqiBqVCLZRUcAXUHv+qGvSQ7q7qqHq3tNnfz6uu6ruufees6t7lfz6u397n2qtBQBgDHab9wAAAFaLwgYAGA2FDQAwGgobAGA0FDYAwGgobACA0Vg/7wGw65tMJi9N8nNJKsnbFxYWfnd6/CVJTktye5IPLyws/Mr8Rgmjs1eSjyXZM0v/X/7+JL+Rpd+5lyV5UJIDk/zLvAYI86Cw4W6ZTCZHZ6moOS7J95J8ZDKZfDjJ4UmemuRhCwsL351MJgfNcZgwRt9NckKSW5LsnuTjSf4yySeSnJvkwrmNDOZozQqbqnpIlv7DdliSluRLST7UWrtmra7JXByV5FMLCwvfSpLJZHJRkpOSPCrJ6QsLC99NkoWFhS3zGyKMUstSUZMsFTa7T49dMbcRwQCsSY9NVb0qyZ9naWrikiSXTr8/u6pevRbXZG6uSvKYyWRywGQy2SfJk5LcN8mRSY6fTCYXTyaTiyaTyaPnOkoYp3VJNifZkuS8JBfPdzgwf7UWt1Soqv+b5Idaa7fd6fgeSa5urR2x6hdlbiaTySlJTs3Svx7/Icm3k/xEkr9O8tIkj07y3iQPXFhYcA8PWH0bkpyT5CVZ+sdGkvxTlpJTPTZ0Za0Km2uT/GRr7fN3On7/JB9trU2287mNSTYmyR//9huO/dmfOXnVx8ba+t3//u4cctC9c8HHP5VTnvOsHPfIhyVJnvDMF+TPNr0199p/w5xHyHKO+SG/d7uiX/jlU/Ltb30n737bWUmSj156Tp71k8/PTV/7xpxHxs64+ssX1yyvd9u/fHZm/9jc/d4PnMnPtlY9Ni9Lcn5VXZfki9Nj90vy4Cx17G9Ta21Tkk3JbP+wuXu++vWbcsD+G3LDjVty/kWfyHv+5HdSVbnk8s057pEPyz994frcdvvt2X/DD8x7qDAa+x+wIbffdntu/uYt2XOvPfMjjzkuZ/zhn857WDB3a1LYtNY+UlVHZmmlzGFZ6q+5PsmlrbU71uKazM/LX/OG3PTNb2b9+vX5tV9+cX5gv3vm6U9+fH79jW/N057zouy++/q88dd/OVUz/YcIjNqBB987b/z912a3dbtlt912y1998PxcdN4n8tM/+6y88NTn5t4H3SvnXHBWPnb+3+Y3fumN8x4uzMyaTEWtBokNzIepKJifmU9FbbludlNRBx0xk5/NzsMAwGjYoA8AetUW5z2CVSexAQBGQ2IDAL1alNgAAAyWxAYAOtX02AAADJfEBgB6pccGAGC4JDYA0Cs9NgAAw6WwAQBGw1QUAPRq8Y55j2DVSWwAgNGQ2ABArzQPAwAMl8QGAHplgz4AgOGS2ABAp9wEEwBgwCQ2ANArPTYAAMMlsQGAXumxAQAYLokNAPTKvaIAAIZLYgMAvdJjAwAwXAobAGA0TEUBQK9s0AcAMFwSGwDoleZhAIDhktgAQK/02AAADJfEBgA61ZpbKgAADJbCBgB61RZn91hGVb20qq6qqqur6mXTY/eqqvOq6rrp1/2XO4/CBgCYq6o6OsnPJTkuycOTPLmqjkjy6iTnt9aOSHL+9PkO6bEBgF4NZ1XUUUk+1Vr7VpJU1UVJTkry1CQ/Nn3PmUkuTPKqHZ1IYgMAzNtVSR5TVQdU1T5JnpTkvkkObq3dkCTTrwctdyKJDQD0aoY7D1fVxiQbtzq0qbW2KUlaa9dU1ZuSnJfkliR/l+T2u3IdhQ0AsOamRcymHbx+RpIzkqSq3pjk+iRfrqpDW2s3VNWhSbYsdx2FDQD0anE4+9hU1UGttS1Vdb8kT0/yI0kekOR5SU6ffv3gcudR2AAAQ/CBqjogyW1JTm2tfb2qTk/yvqo6JckXkjxzuZMobACAuWutHb+NY19N8ridOY/CBgB6NcPm4Vmx3BsAGA2JDQD0ajgb9K0aiQ0AMBoSGwDolR4bAIDhktgAQK/02AAADJfEBgB6JbEBABguiQ0AdKq14dwEc7VIbACA0ZDYAECv9NgAAAyXxAYAemXnYQCA4VLYAACjYSoKAHqleRgAYLgkNgDQK83DAADDJbEBgF7psQEAGC6JDQD0So8NAMBwSWwAoFd6bAAAhktiAwC9ktgAAAyXxAYAemVVFADAcElsAKBXemwAAIZLYQMAjIapKADoleZhAIDhktgAQK80DwMADJfEBgB6pccGAGC4JDYA0Cs9NgAAwyWxAYBeSWwAAIZLYgMAvWpt3iNYdRIbAGA0JDYA0Cs9NgAAwyWxAYBeSWwAAIZLYgMAvXKvKACA4VLYAACjYSoKAHqleRgAYLgkNgDQK7dUAAAYLokNAPRKjw0AwHBJbACgVxIbAIDhktgAQK/cUgEAYLgkNgDQqbZoHxsAgMGS2ABAr6yKAgAYLokNAPTKqigAgOFS2AAAo2EqCgB6Zbk3AMBwSWwAoFeWewMADJfEBgB6JbEBABguiQ0A9KpZFQUAMFgSGwDolR4bAIDhktgAQK/sPAwAMFwSGwDoVdNjAwAwWBIbAOiVHhsAgOFS2AAAo2EqCgA61WzQBwAwXBIbAOiV5mEAgOGS2ABAr2zQBwAwXBIbAOiVHhsAgOGS2ABAr+xjAwAwXBIbAOiVHhsAgOGS2ABAr+xjAwAwXBIbAOiVHhsAgOGS2AAAc1VVkyTv3erQA5O8NsmGJD+X5CvT469prf3vHZ1LYQMAnWoD2aCvtbaQ5Jgkqap1Sf45yTlJXpDkra21t6z0XKaiAIAheVySz7TWPn9XPqywAYBeLbbZPVbu2UnO3ur5aVV1ZVW9s6r2X+7DChsAYM1V1caqumyrx8ZtvGePJCcm+YvpobcleVCWpqluSPLby11Hjw0A9GqGy71ba5uSbFrmbU9M8unW2penn/ny91+oqrcnOXe560hsAIChODlbTUNV1aFbvXZSkquWO4HEBgB6NaBbKlTVPkl+IsnPb3X4t6rqmCQtyT/d6bVtUtgAAHPXWvtWkgPudOy5O3sehQ0A9MotFQAAhktiAwCdahIbAIDhktgAQK8kNgAAwyWxAYBeDeTu3qtJYgMAjIbCBgAYDVNRANArzcMAAMMlsQGAXklsAACGS2IDAJ1qTWIDADBYEhsA6JUeGwCA4ZLYAECvJDYAAMMlsQGATjWJDQDAcElsAKBXEhsAgOGS2ABArxbnPYDVJ7EBAEZDYQMAjIapKADolOXeAAADJrEBgF5JbAAAhktiAwC9stwbAGC4JDYA0CmrogAABkxiAwC90mMDADBcEhsA6JQeGwCAAZPYAECv9NgAAAyXxAYAOtUkNgAAw6WwAQBGw1QUAPTKVBQAwHBJbACgU5qHAQAGTGIDAL2S2AAADJfEBgA6pccGAGDAJDYA0CmJDQDAgElsAKBTEhsAgAGT2ABAr1rNewSrTmIDAIyGxAYAOqXHBgBgwBQ2AMBomIoCgE61Rc3DAACDJbEBgE5pHgYAGDCJDQB0qtmgDwBguCQ2ANApPTYAAAMmsQGATtnHBgBgwCQ2ANCp1uY9gtUnsQEARkNiAwCd0mMDADBgEhsA6JTEBgBgwBQ2AMBomIoCgE5Z7g0AMGASGwDolOZhAIABk9gAQKdak9gAAAyWxAYAOtUW5z2C1SexAQBGQ2IDAJ1a1GMDADBcEhsA6JRVUQAAAyaxAYBO2XkYAGDAJDYA0Cl39wYAGDCFDQAwGqaiAKBTmocBAAZMYgMAnRrjLRW2W9hU1f9Kst1+6dbaiWsyIgCAu2hHic1bZjYKAGDmxnhLhe0WNq21i2Y5EACAu2vZHpuqOiLJbyZ5aJK9vn+8tfbANRwXALDGet2g711J3pbk9iSPTfKnSf7HWg4KAOCuWElhs3dr7fwk1Vr7fGvtdUlOWNthAQBrbbHVzB7LqaoNVfX+qrq2qq6pqh+pqntV1XlVdd306/7LnWclhc13qmq3JNdV1WlVdVKSg1bwOQCAlfq9JB9prT0kycOTXJPk1UnOb60dkeT86fMdWklh87Ik+yT5xSTHJnlukufdxUEDAAPRWs3ssSNVtV+SxyQ5Y2lc7XuttZuSPDXJmdO3nZnkacv9TMs2D7fWLp1+e0uSFyz3fgCAnfTAJF9J8q6qeniSy5O8NMnBrbUbkqS1dkNVLTtjtJJVURdkGxv1tdb02QDALmyWq6KqamOSjVsd2tRa2zT9fn2SRyZ5SWvt4qr6vaxg2mlbVnJLhVds9f1eSZ6RpRVSAAArMi1iNm3n5euTXN9au3j6/P1ZKmy+XFWHTtOaQ5NsWe46K5mKuvxOhz5RVTbvA4Bd3FDuFdVau7GqvlhVk9baQpLHJfmH6eN5SU6ffv3gcudayVTUvbZ6uluWGogPuSsDBwDYjpckOauq9kjy2Sz19e6W5H1VdUqSLyR55nInWclU1OVZ6rGpLE1BfS7JKXdx0Cu2932OX+tLANtwyyf/aN5DAGZkSPeKaq1tTvKobbz0uJ05z0oKm6Naa9/Z+kBV7bkzFwEAmIWV7GPzt9s49snVHggAwN213cSmqg5JcliSvavqEVmaikqS/bK0YR8AsAsbSvPwatrRVNRPJnl+ksOT/Hb+rbD5ZpLXrO2wAAB23nYLm9bamUnOrKpntNY+MMMxAQAzMMP9+WZmJT02x1bVhu8/qar9q+oNazgmAIC7ZCWFzROnN6JKkrTWvp7kSWs3JABgFhZbzewxKyspbNZtvby7qvZOYrk3ADA4K9nH5j1Jzq+qd02fvyD/dgtxAGAXNaQN+lbLSu4V9VtVdWWSH8/SyqiPJLn/Wg8MAGBnrSSxSZIbkywmeVaWbqlglRQA7OIW5z2ANbCjDfqOTPLsJCcn+WqS9yap1tpjZzQ2AICdsqPE5tokf5PkKa21f0ySqnr5TEYFAKy5lvH12OxoVdQzsjQFdUFVvb2qHpeM8E8AABiNHe08fE6Sc6rqHkmeluTlSQ6uqrclOae19tEZjREAWAOLI9x6eNl9bFprt7bWzmqtPTlL943anOTVaz4yAICdtNJVUUmS1trXkvzJ9AEA7MIWR9hhspKdhwEAdgkKGwBgNHZqKgoAGI/elnsDAOxSJDYA0Kkx3lJBYgMAjIbEBgA6pccGAGDAJDYA0Ck9NgAAAyaxAYBOSWwAAAZMYgMAnbIqCgBgwCQ2ANCpxfEFNhIbAGA8JDYA0KlFPTYAAMOlsAEARsNUFAB0qs17AGtAYgMAjIbEBgA65ZYKAAADJrEBgE4tluXeAACDJbEBgE5ZFQUAMGASGwDolFVRAAADJrEBgE4tjm9RlMQGABgPiQ0AdGox44tsJDYAwGhIbACgU/axAQAYMIUNADAapqIAoFOWewMADJjEBgA65ZYKAAADJrEBgE5Z7g0AMGASGwDolFVRAAADJrEBgE5ZFQUAMGASGwDolMQGAGDAJDYA0KlmVRQAwHBJbACgU3psAAAGTGEDAIyGqSgA6JSpKACAAZPYAECn2rwHsAYkNgDAaEhsAKBTizboAwAYLokNAHTKqigAgAGT2ABApyQ2AAADJrEBgE7ZxwYAYMAkNgDQKfvYAAAMmMQGADplVRQAwIApbACA0TAVBQCdstwbAGDAJDYA0KnFEWY2EhsAYDQkNgDQKcu9AQAGTGIDAJ0aX4eNxAYAGBGJDQB0So8NAMCASWwAoFOLNe8RrD6JDQAwGhIbAOiUnYcBAAZMYgMAnRpfXiOxAQBGRGEDAAxCVa2rqiuq6tzp83dX1eeqavP0ccxy5zAVBQCdGuAGfS9Nck2S/bY69srW2vtXegKJDQAwd1V1eJL/lOQdd+c8ChsA6NRi2sweK/C7SX4l/z5I+m9VdWVVvbWq9lzuJAobAGDNVdXGqrpsq8fGrV57cpItrbXL7/SxX03ykCSPTnKvJK9a7jp6bACgU7Nc7t1a25Rk03Ze/tEkJ1bVk5LslWS/qnpPa+0509e/W1XvSvKK5a4jsQEA5qq19quttcNbaz+Y5NlJ/rq19pyqOjRJqqqSPC3JVcudS2IDAJ0a4KqoOzurqg5MUkk2J3nRch9Q2AAAg9FauzDJhdPvT9jZzytsAKBTboIJADBgEhsA6NT48hqJDQAwIhIbAOjULrAqaqdJbACA0ZDYAECn2gi7bCQ2AMBoKGwAgNEwFQUAndI8DAAwYBIbAOiUWyoAAAyYxAYAOjW+vEZiAwCMiMQGADqlxwYAYMAkNgDQKfvYAAAMmMQGADrlJpgAAAMmsQGATumxAQAYMIkNAHRKjw0AwIApbACA0TAVBQCd0jwMADBgEhsA6NRi0zwMADBYEhsA6NT48hqJDQAwIhIbAOjU4ggzG4kNADAaEhsA6JRbKgAADJjEBgA6ZedhAIABk9gAQKesigIAGDCJDQB0yqooAIABU9gAAKNhKgoAOmW5NwDAgElsAKBTrWkeBgAYLIkNAHTKBn0AAAMmsQGATlkVBQAwYBIbAOiUWyoAAAyYxAYAOmVVFADAgElsAKBTdh4GABgwiQ0AdMo+NgAAAyaxAYBO2ccGAGDAFDYAwGiYigKATtmgDwBgwCQ2ANApG/QBAAyYxAYAOqXHBgBgwCQ2ANApG/QBAAyYxAYAOrVoVRQAwHBJbACgU+PLayQ2AMCISGwAoFP2sQEAGDCJDQB0SmIDADBgChsAYDRMRQFAp5oN+gAAhktiAwCd0jwMADBgEhsA6FST2AAADJfEBgA6ZVUUAMCASWwAoFNWRQEADJjEBgA6pccGAGDAJDYA0Ck9NgAAAyaxAYBO2XkYAGDAFDYAwGiYigKATi1a7g0AMFwSGwDolOZhAIABk9gAQKf02AAADJjEBgA6pccGAGCVVdVeVXVJVf1dVV1dVa+fHn9AVV1cVddV1Xurao/lzqWwAYBOLbY2s8cyvpvkhNbaw5Mck+QJVfXDSd6U5K2ttSOSfD3JKcudSGEDAMxVW3LL9Onu00dLckKS90+Pn5nkacudS2EDAJ1qM/zfcqpqXVVtTrIlyXlJPpPkptba7dO3XJ/ksOXOo7ABANZcVW2sqsu2emzc+vXW2h2ttWOSHJ7kuCRHbeM0y1ZIVkUBQKdmuY9Na21Tkk0reN9NVXVhkh9OsqGq1k9Tm8OTfGm5z0tsAIC5qqoDq2rD9Pu9k/x4kmuSXJDkp6Zve16SDy53LokNAHRqQPvYHJrkzKpal6XQ5X2ttXOr6h+S/HlVvSHJFUnOWO5EChsAYK5aa1cmecQ2jn82S/02K2YqCgAYDYkNAHSqtcV5D2HVSWwAgNGQ2ABApxaH0zy8aiQ2AMBoSGwAoFNthhv0zYrEBgAYDYkNAHRKjw0AwIBJbACgU3psAAAGTGIDAJ1alNgAAAyXxAYAOtWsigIAGC6JDQB0yqooAIABU9gAAKNhKgoAOuWWCgAAAyaxAYBOaR4GABgwiQ0AdMotFQAABkxiAwCd0mMDADBgEhsA6JR9bAAABkxiAwCd0mMDADBgEhsA6JR9bAAABkxiAwCdalZFAQAMl8IGABgNU1EA0CnNwwAAAyaxAYBO2aAPAGDAJDYA0CnLvQEABkxiAwCdGmOPjcKGu+Xww++Td7/z93LwIQdmcXEx73jHWfmDPzwjr3/dK/OUpzw+i4stX9nyL3nhz748N9zw5XkPF0blrL/8eD5wwSVpreUZJxyX5zzx+Hzjlm/lV37/rHzpK1/PfQ7cP2/+xZ/OfvvuM++hwszUUKu19XscNsyB8f855JCDcughB+WKzVdl333vkUsu/kie8VMvzPXX35Cbb74lSXLaqS/MUUcdmVNPe/WcR8tK3PLJP5r3EFiB6754Y171B3+Ws/7radl9/bq8+PR35tdeeFL+5wWXZL99984pJz42Z3zognzz1m/n5Sc/ad7DZYX2OvZpNcvr7T7D/9be9r1/nsnPpseGu+XGG7fkis1XJUluueXWXHvtdTnsPof8a1GTJPe4xz6jjDthnj73z1vysAffL3vvuUfWr1uXY496QP76sqtyweVX58Tjj02SnHj8sbngsqvnPFKYrZkXNlX1gllfk9m4//0PzzEPPzoXX3JFkuS//pdX5XOfuTQnn3xSXvf6N895dDAuD77vwbn82s/lpptvzbe/+718fPNCbvzqN/K1b9ySA/ffL0ly4P775WvfuHXOI2XI2gwfszKPxOb1c7gma+we99gn73vv2/NLr/iNf01r/vNr35QHPOjROfvsc3Lqi9WzsJoeeNjBecFT/mN+/jffkRe/6Z058v6HZv06ITysSY9NVV25vZeSHNla23M7n9uYZOP06abW2qZVHxxrYfck5yb5qyS/kyz9XW7193f/JB9OcvR8hgfjN5lM3pjk+jvuuOO169ate8TCwsINk8nk0CQXLiwsTOY9PpiVtSrvD07yM0meso3HV7f3odbaptbao6YPRc2uoZKckeSaTIuaJDn66KNfstV7Tkxy7YzHBaM3mUwOmn69X5KnJzn75ptvTpLnTd/yvCQfnM/oYD7Warn3uUn2ba1tvvMLVXXhGl2T+fjRJM9N8vdJvv/3/Zq3vOUthye5Ksliks8nedF8hgej9oHJZHJAktuSnLqwsPD19evX37Bhw4afmEwmpyT5QpJnzneIMFuDXe7Nrq2qLmutPWre44De+N2jdzrNWCumEmE+/O7RNYkNADAaEhsAYDQUNqyqqnpCVS1U1T9WlXsowIxU1TuraktVXTXvscA8KWxYNVW1LskfJXlikocmObmqHjrfUUE33p3kCfMeBMybwobVdFySf2ytfba19r0kf57kqXMeE3ShtfaxJF+b9zhg3hQ2rKbDknxxq+fXT48BwEwobFhN27olvWV3AMyMwobVdH2S+271/PAkX5rTWADokMKG1XRpkiOq6gFVtUeSZyf50JzHBEBHFDasmtba7UlOy9Jdvq9J8r7W2tXzHRX0oarOTvLJJJOqur6qTpn3mGAe7DwMAIyGxAYAGA2FDQAwGgobAGA0FDYAwGgobACA0VDYwC6qqu6oqs1VdVVV/UVV7XM3zvVjVXXu9PsTd3Rn9qraUFUvvgvXeF1VveKujhFgJRQ2sOv6dmvtmNba0Um+l+RFW79YS3b6d7y19qHW2uk7eMuGJDtd2ADMgsIGxuFvkjy4qn6wqq6pqj9O8ukk962qx1fVJ6vq09NkZ98kqaonVNW1VfXxJE///omq6vlV9YfT7w+uqnOq6u+mj/+Q5PQkD5qmRW+evu+VVXVpVV1ZVa/f6ly/VlULVfV/kkxm9qcBdEthA7u4qlqf5IlJ/n56aJLkT1trj0hya5JfT/LjrbVHJrksyS9V1V5J3p7kKUmOT3LIdk7/+0kuaq09PMkjk1yd5NVJPjNNi15ZVY9PckSS45Ick+TYqnpMVR2bpdtqPCJLhdOjV/lHB/h31s97AMBdtndVbZ5+/zdJzkhynySfb619anr8h5M8NMknqipJ9sjStvsPSfK51tp1SVJV70mycRvXOCHJzyRJa+2OJN+oqv3v9J7HTx9XTJ/vm6VC555JzmmtfWt6DfcNA9acwgZ2Xd9urR2z9YFp8XLr1oeSnNdaO/lO7zsmyWrdT6WS/GZr7U/udI2XreI1AFbEVBSM26eS/GhVPThJqmqfqjoyybVJHlBVD5q+7+TtfP78JL8w/ey6qtovyc1ZSmO+76+SvHCr3p3DquqgJB9LclJV7V1V98zStBfAmlLYwIi11r6S5PlJzq6qK7NU6DyktfadLE09fXjaPPz57ZzipUkeW1V/n+TyJD/UWvtqlqa2rqqqN7fWPprkz5J8cvq+9ye5Z2vt00nem2Rzkg9kaboMYE25uzcAMBoSGwBgNBQ2AMBoKGwAgNFQ2AAAo6GwAQBGQ2EDAIyGwgYAGA2FDQAwGv8PwYpj94xtVmAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "params_lr = {'logisticregression__C': [0.001,0.01,0.1,1,10,100,1000],\n",
    "             'logisticregression__penalty': ['l1','l2'],\n",
    "             \n",
    "          }\n",
    "lr = make_pipeline(LogisticRegression())\n",
    "\n",
    "accuracies['LogisticRegression(Embedding)'] = grid_search_best_params('LogisticRegression_Embedding', lr, params_lr,xtrain_glove,y_train_sentence,xvalid_glove,y_test_sentence,x_train_all,y_train_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conf_matrix(conf_mat):\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    sns.heatmap(conf_mat, annot=True, fmt='d')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show()\n",
    "\n",
    "def grid_search_best_params(model_name, pipeline, params, xtrain, ytrain, xvalid, yvalid):\n",
    "    grid = GridSearchCV(pipeline, param_grid=params, cv=5, n_jobs=1, refit='accuracy', scoring=['accuracy'])\n",
    "    grid.fit(xtrain, ytrain)\n",
    "    grids[model_name] = copy.deepcopy(grid)\n",
    "    y_pred_test = grid.predict(xvalid)\n",
    "    y_test_proba = grid.predict_proba(xvalid)\n",
    "    \n",
    "    vec_type = 'TF-IDF Vectorizer' if 'TF-IDF' in model_name else 'CountVectorizer'\n",
    "    print(\"\\nLog Loss for %s on Test Set for %s : %s\"%(model_name,vec_type,log_loss(yvalid, y_pred_test)))\n",
    "    print(\"ROC AUC  for %s on Test Set for %s : %s\"%(model_name,vec_type,roc_auc_score(yvalid, y_test_proba.T[1])))\n",
    "    print(\"Accuracy for %s on Test Set for %s : %s\"%(model_name,vec_type,accuracy_score(yvalid, y_pred_test)))\n",
    "    \n",
    "    \n",
    "    y_pred_train=grid.predict(xtrain)\n",
    "    y_train_proba = grid.predict_proba(xtrain)\n",
    "    print(\"Log Loss for %s on Training %s : %s\"%(model_name,vec_type,log_loss(ytrain, y_pred_train)))\n",
    "    print(\"ROC AUC for %s on Training %s : %s\"%(model_name,vec_type,roc_auc_score(ytrain, y_train_proba.T[1])))\n",
    "    print(\"Accuracy for %s on Training %s : %s\"%(model_name, vec_type,accuracy_score(ytrain,y_pred_train)))\n",
    " \n",
    "    print('Best Score : %.3f%%'%(grid.best_score_*100))\n",
    "    print('Best Paramerter Settings : ',grid.best_params_)\n",
    "    \n",
    "      \n",
    "    conf_mat = confusion_matrix(yvalid, y_pred_test)\n",
    "    plot_conf_matrix(conf_mat)\n",
    "    \n",
    "    accuracies = {'Val ROC AUC': roc_auc_score(yvalid, y_test_proba.T[1]),\n",
    "                  'Val Log Loss':log_loss(yvalid, y_pred_test),\n",
    "                  'Val Accuracy':accuracy_score(yvalid, y_pred_test),               \n",
    "                  'Best Accuracy': grid.best_score_,\n",
    "                  'Best Param Settings' :grid.best_params_,\n",
    "                  }\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lr = {'mlpclassifier__hidden_layer_sizes': [(32, 64, 128) ,(64, 128, 256)],\n",
    "             'mlpclassifier__activation': ['relu'],\n",
    "             }\n",
    "lr = make_pipeline(MLPClassifier())\n",
    "\n",
    "accuracies['NeuralNet(Embedding)'] = grid_search_best_params('NeuralNet(Embedding)', lr, params_lr,xtrain_glove,y_train_sentence,xvalid_glove,y_test_sentence,x_train_all,y_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lr = {'mlpclassifier__hidden_layer_sizes': [(32, 64, 128) ,(64, 128, 256)],\n",
    "             'mlpclassifier__activation': ['logistic', 'tanh', 'relu'],\n",
    "             'mlpclassifier__alpha' : [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "             }\n",
    "lr = make_pipeline(MLPClassifier())\n",
    "\n",
    "accuracies['NeuralNet(Embedding)'] = grid_search_best_params('NeuralNet(Embedding)', lr, params_lr,xtrain_glove,y_train_sentence,xvalid_glove,y_test_sentence,x_train_all,y_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o7oYZ4tYB79z"
   },
   "source": [
    "### Fitting a simple Random Forest on the Glove features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OoAfCyntBkES"
   },
   "source": [
    "## Random forests \n",
    "*  Random forests is a decision tree based algorithm used to solve regression and classification problems. An inverted tree is framed which is branched off from a homogeneous probability distributed root node, to highly heterogeneous leaf nodes, for deriving the output. \n",
    "*  Decision tree handles colinearity better than LR.\n",
    "*  Decision trees cannot derive the significance of features, but LR can.\n",
    "*  Decision trees are better for categorical values than LR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrt_features = int(np.sqrt(x_train_all.shape[1]))\n",
    "params_rf = {'randomforestclassifier__n_estimators': [10],\n",
    "             \n",
    "            }\n",
    "rf = make_pipeline(RandomForestClassifier())\n",
    "\n",
    "accuracies['RandomForest(Embedding)'] = grid_search_best_params('RandomForest_Embedding',rf, params_rf,xtrain_glove,y_train_sentence,xvalid_glove,y_test_sentence,x_train_all,y_train_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fixed ready to train random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrt_features = int(np.sqrt(x_train_all.shape[1]))\n",
    "params_rf = {'randomforestclassifier__n_estimators': [1, 8, 32, 64, 100, 200, 500],\n",
    "             'randomforestclassifier__min_samples_leaf': [0.1,0.2,0.3,0.4,0.5,1],\n",
    "             'randomforestclassifier__max_depth' : [None,3,4,5],\n",
    "             'randomforestclassifier__bootstrap' : [False],\n",
    "            }\n",
    "rf = make_pipeline(RandomForestClassifier())\n",
    "\n",
    "accuracies['RandomForest(Embedding)'] = grid_search_best_params('RandomForest_Embedding',rf, params_rf,xtrain_glove,y_train_sentence,xvalid_glove,y_test_sentence,x_train_all,y_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x='model_name',y=['Train Accuracy', 'Test Accuracy','Best Accuracy'], kind='bar', figsize=(8,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x='model_name',y=['Train Log Loss', 'Test Log Loss'], kind='bar', figsize=(8,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x='model_name',y=['Train ROC AUC', 'Test ROC AUC'], kind='bar', figsize=(8,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in grids.items():\n",
    "    print(model_name + ' 5 Fold Log Loss(Error)')\n",
    "    df_lr = pd.DataFrame(model.cv_results_)\n",
    "    df_lr = df_lr[df_lr.params == model.best_params_]\n",
    "    errors = df_lr[['split0_test_neg_log_loss','split1_test_neg_log_loss','split2_test_neg_log_loss','split3_test_neg_log_loss','split4_test_neg_log_loss','mean_test_neg_log_loss']].values[0]\n",
    "    print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in grids.items():\n",
    "    print(model_name + '5 Fold ROC AUC')\n",
    "    df_lr = pd.DataFrame(model.cv_results_)\n",
    "    df_lr = df_lr[df_lr.params == model.best_params_]\n",
    "    roc_aucs = df_lr[['split0_test_roc_auc','split1_test_roc_auc','split2_test_roc_auc','split3_test_roc_auc','split4_test_roc_auc','mean_test_roc_auc']].values[0]\n",
    "    print(roc_aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in grids.items():\n",
    "    print(model_name+' 5 Fold Accuracies')\n",
    "    df_lr = pd.DataFrame(model.cv_results_)\n",
    "    df_lr = df_lr[df_lr.params == model.best_params_]\n",
    "    accuracies = df_lr[['split0_test_accuracy','split1_test_accuracy','split2_test_accuracy','split3_test_accuracy','split4_test_accuracy','mean_test_accuracy']].values[0]\n",
    "    print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv('x_train.csv')\n",
    "y_train = pd.read_csv('y_train.csv')\n",
    "df=pd.concat([x_train,y_train], axis=1)\n",
    "df['Sentiment'] = np.where(df['is_positive_sentiment']== 1, 'Positive', 'Negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "col = ['Sentiment', 'text']\n",
    "df = df[col]\n",
    "df = df[pd.notnull(df['text'])]\n",
    "df.columns = ['Sentiment', 'text']\n",
    "df['Sentiment_id'] = df['Sentiment'].factorize()[0]\n",
    "Sentiment_id_df = df[['Sentiment', 'Sentiment_id']].drop_duplicates().sort_values('Sentiment_id')\n",
    "Sentiment_to_id = dict(Sentiment_id_df.values)\n",
    "id_to_Sentiment = dict(Sentiment_id_df[['Sentiment_id', 'Sentiment']].values)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IH1IzEv2BkEV"
   },
   "source": [
    "## How out of vocab words are handled in word embeddings model\n",
    "*  adding an unknown word token is how most people solve this problem.\n",
    "*  deleting the unknown words is a bad idea because it transforms the sentence in a way that is not consistent with how the model was trained.\n",
    "*  Another option that has recently been developed is to create a word embedding on-the-fly for each word using a convolutional neural network or a separate LSTM that processes the characters of each word one at a time. Using this technique your model will never encounter a word that it can't create an embedding for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bjhOdjLrBwSM"
   },
   "source": [
    "# Intuition on L2 regularization\n",
    "*  In L2 regularization, regularization term is the sum of square of all feature weights as shown above in the equation.\n",
    "*  L2 regularization forces the weights to be small but does not make them zero and does non sparse solution.\n",
    "*  L2 is not robust to outliers as square terms blows up the error differences of the outliers and the regularization term tries to fix it by penalizing the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rq2VHJx7DqwK"
   },
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "6kXpKc5fMAUh",
    "outputId": "31610a6f-58fb-4c93-d6a7-8950b9451bcb"
   },
   "outputs": [],
   "source": [
    "###### Comparisions Models ##########\n",
    "\n",
    "models = [\n",
    "    RandomForestClassifier(bootstrap=False, n_estimators= 300, criterion='gini'),\n",
    "    xgb.XGBClassifier(nthread=10, silent=False),\n",
    "    LogisticRegression(C=1,penalty=\"l2\"),\n",
    "]\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []\n",
    "for model in models:\n",
    "  model_name = model.__class__.__name__\n",
    "  accuracies = cross_val_score(model, xtrain_glove, y_train ,scoring='accuracy', cv=CV)\n",
    "  for fold_idx, accuracy in enumerate(accuracies):\n",
    "    entries.append((model_name, fold_idx, accuracy))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HOW TO DO IT? No polished figure, table OR explanation showing which features are most important to the best performing classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CwIvEWN2BkEy"
   },
   "source": [
    "## Box plot\n",
    "A box plot shows the distribution of quantitative data in a way that facilitates comparisons between variables or across levels of a categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "colab_type": "code",
    "id": "VAoGT3tUMAUk",
    "outputId": "5479d31e-e891-4594-fd98-c75d6dcad0fa"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n",
    "sns.stripplot(x='model_name', y='accuracy', data=cv_df, \n",
    "              size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rx3b5Q7SD6W5"
   },
   "source": [
    "Comparing the accuracies of various models we so far trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "LOlv0Aa1MAUn",
    "outputId": "f678fcb1-d566-4103-a5f6-eb9db99a6777"
   },
   "outputs": [],
   "source": [
    "cv_df.groupby('model_name').accuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, grid in grids.items():\n",
    "    print(model_name)\n",
    "    print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEED TO FIX below not countvector or tfidf TO THIS MODEL: OTHERWISE IT IS WRONG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_coefficients(classifier, feature_names, n_top_features=25):\n",
    "    # get coefficients with large absolute values \n",
    "    coef = classifier.coef_.ravel()\n",
    "    positive_coefficients = np.argsort(coef)[-n_top_features:]\n",
    "    negative_coefficients = np.argsort(coef)[:n_top_features]\n",
    "    interesting_coefficients = np.hstack([negative_coefficients, positive_coefficients])\n",
    "    # plot them\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    colors = [\"red\" if c < 0 else \"blue\" for c in coef[interesting_coefficients]]\n",
    "    plt.bar(np.arange(50), coef[interesting_coefficients], color=colors)\n",
    "    feature_names = np.array(feature_names)\n",
    "    plt.xticks(np.arange(1, 51), feature_names[interesting_coefficients], rotation=60, ha=\"right\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######Count vector Creation##############\n",
    "ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 1), stop_words = 'english')\n",
    "\n",
    "# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\n",
    "ctv.fit(list(xtrain1) + list(xvalid))\n",
    "xtrain_ctv =  ctv.transform(xtrain1) \n",
    "xvalid_ctv = ctv.transform(xvalid)\n",
    "lr = LogisticRegression(C=1, penalty='l2')\n",
    "lr.fit(xtrain_ctv,ytrain1)\n",
    "\n",
    "visualize_coefficients(lr, ctv.get_feature_names())\n",
    "plt.title('Logistic Regression with Count Vectorizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in grids.items():\n",
    "    print(model_name + ' 5 Fold Log Loss(Error)')\n",
    "    df_lr = pd.DataFrame(model.cv_results_)\n",
    "    df_lr = df_lr[df_lr.params == model.best_params_]\n",
    "    errors = df_lr[['split0_test_neg_log_loss','split1_test_neg_log_loss','split2_test_neg_log_loss','split3_test_neg_log_loss','split4_test_neg_log_loss','mean_test_neg_log_loss']].values[0]\n",
    "    print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in grids.items():\n",
    "    print(model_name + '5 Fold ROC AUC')\n",
    "    df_lr = pd.DataFrame(model.cv_results_)\n",
    "    df_lr = df_lr[df_lr.params == model.best_params_]\n",
    "    roc_aucs = df_lr[['split0_test_roc_auc','split1_test_roc_auc','split2_test_roc_auc','split3_test_roc_auc','split4_test_roc_auc','mean_test_roc_auc']].values[0]\n",
    "    print(roc_aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in grids.items():\n",
    "    print(model_name+' 5 Fold Accuracies')\n",
    "    df_lr = pd.DataFrame(model.cv_results_)\n",
    "    df_lr = df_lr[df_lr.params == model.best_params_]\n",
    "    accuracies = df_lr[['split0_test_accuracy','split1_test_accuracy','split2_test_accuracy','split3_test_accuracy','split4_test_accuracy','mean_test_accuracy']].values[0]\n",
    "    print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bUiV9kfPMAUr"
   },
   "outputs": [],
   "source": [
    "x_train = pd.read_csv('x_train.csv')\n",
    "y_train = pd.read_csv('y_train.csv')\n",
    "df=pd.concat([x_train,y_train], axis=1)\n",
    "df['Sentiment'] = np.where(df['is_positive_sentiment']== 1, 'Positive', 'Negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "CV=5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(grids)))\n",
    "entries = []\n",
    "for model_name, model in grids.items():\n",
    "    #model_name = model.__class__.__name__\n",
    "    accuracies = df_lr = pd.DataFrame(model.cv_results_)\n",
    "    df_lr = df_lr[df_lr.params == model.best_params_]\n",
    "    #print(df_lr.columns)\n",
    "    accuracies = df_lr[['split0_test_accuracy','split1_test_accuracy','split2_test_accuracy','split3_test_accuracy','split4_test_accuracy','mean_test_accuracy']].values[0]\n",
    "    for fold_idx, accuracy in enumerate(accuracies):\n",
    "        entries.append((model_name, fold_idx, accuracy))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "\n",
    "sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n",
    "sns.stripplot(x='model_name', y='accuracy', data=cv_df, \n",
    "              size=8, jitter=True, edgecolor=\"gray\", linewidth=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "cv_df = pd.DataFrame(index=range(CV * len(grids)))\n",
    "entries = []\n",
    "for model_name, model in grids.items():\n",
    "    #model_name = model.__class__.__name__\n",
    "    df_lr = pd.DataFrame(model.cv_results_)\n",
    "    df_lr = df_lr[df_lr.params == model.best_params_]\n",
    "    #print(df_lr.columns)\n",
    "    roc_aucs = df_lr[['split0_test_roc_auc','split1_test_roc_auc','split2_test_roc_auc','split3_test_roc_auc','split4_test_roc_auc','mean_test_roc_auc']].values[0]\n",
    "    for fold_idx, roc_auc in enumerate(roc_aucs):\n",
    "        entries.append((model_name, fold_idx, roc_auc))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'roc_auc'])\n",
    "\n",
    "sns.boxplot(x='model_name', y='roc_auc', data=cv_df)\n",
    "sns.stripplot(x='model_name', y='roc_auc', data=cv_df, \n",
    "              size=8, jitter=True, edgecolor=\"gray\", linewidth=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "MkeQcC9-MAUt",
    "outputId": "1977756f-72cf-4a13-e530-5784edca8573"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "g5LpGPKqMAU0",
    "outputId": "22e8a091-156a-44ff-c639-d67e40052162"
   },
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "col = ['Sentiment', 'text']\n",
    "df = df[col]\n",
    "df = df[pd.notnull(df['text'])]\n",
    "df.columns = ['Sentiment', 'text']\n",
    "df['Sentiment_id'] = df['Sentiment'].factorize()[0]\n",
    "Sentiment_id_df = df[['Sentiment', 'Sentiment_id']].drop_duplicates().sort_values('Sentiment_id')\n",
    "Sentiment_to_id = dict(Sentiment_id_df.values)\n",
    "id_to_Sentiment = dict(Sentiment_id_df[['Sentiment_id', 'Sentiment']].values)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv('x_train.csv')\n",
    "xtrain_glove_whole = [sent2vec(x) for x in tqdm(x_train['text'])]\n",
    "xtrain_glove_whole = np.array(xtrain_glove_whole)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.read_csv('y_train.csv')\n",
    "y = y_train['is_positive_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "for predicted in Sentiment_id_df.Sentiment_id:\n",
    "  for actual in Sentiment_id_df.Sentiment_id:\n",
    "    if predicted != actual and conf_mat[actual, predicted] >= 10:\n",
    "      print(\"'{}' predicted as '{}' : {} examples.\".format(id_to_Sentiment[actual], id_to_Sentiment[predicted], conf_mat[actual, predicted]))\n",
    "      display(df.loc[indices_test[(y_test == actual) & (y_pred == predicted)]][['Sentiment', 'text']])\n",
    "      print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "nGJJdq6LMAU2",
    "outputId": "f7debfee-dede-4691-f507-4cdc8f0c4d9f"
   },
   "outputs": [],
   "source": [
    "x_train = pd.read_csv('x_train.csv')\n",
    "xtrain_glove_whole = [sent2vec(x) for x in tqdm(x_train['text'])]\n",
    "xtrain_glove_whole = np.array(xtrain_glove_whole)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_glove = [sent2vec(x) for x in tqdm(x_test['text'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "29LBVCD7XFZb"
   },
   "source": [
    "### we will print out all the negatives makred positve and all positives marked negative below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 889
    },
    "colab_type": "code",
    "id": "JfWFxnutMAVB",
    "outputId": "804fecf6-bbbc-4603-ad25-0ef9f30d6b09"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "for predicted in Sentiment_id_df.Sentiment_id:\n",
    "  for actual in Sentiment_id_df.Sentiment_id:\n",
    "    if predicted != actual and conf_mat[actual, predicted] >= 10:\n",
    "      print(\"'{}' predicted as '{}' : {} examples.\".format(id_to_Sentiment[actual], id_to_Sentiment[predicted], conf_mat[actual, predicted]))\n",
    "      display(df.loc[indices_test[(y_test == actual) & (y_pred == predicted)]][['Sentiment', 'text']])\n",
    "      print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_AgPq9fpXVTI"
   },
   "source": [
    "### Testing trained model on the testing set defined at the begining. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rAh7YowbXkk9"
   },
   "source": [
    "Creating word vectors of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "jxcTx7ydMAVH",
    "outputId": "f8c3a022-d9d9-4c6b-fc44-c5b9ab2c6fc8"
   },
   "outputs": [],
   "source": [
    "x_test_glove = [sent2vec(x) for x in tqdm(x_test['text'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rt4P9XCRXi7r"
   },
   "source": [
    "Using model to predict the probability of sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w678kfp2IoR8"
   },
   "source": [
    "### Hyperparameter Instructions\n",
    "*  We have intialised the hidden layer values with two different sets (32, 64, 128)\n",
    "*  The grid search is programmed to apply activation functions such as logistic regression, tanh and relu \n",
    "*  We have also vectorised input using Tf-idf algorithm and set the n grams to range from (1, 1) to (2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5AahaSEeWTMj"
   },
   "source": [
    "## Random forests \n",
    "*  Random forests is a decision tree based algorithm used to solve regression and classification problems. An inverted tree is framed which is branched off from a homogeneous probability distributed root node, to highly heterogeneous leaf nodes, for deriving the output. \n",
    "*  Decision tree handles colinearity better than LR.\n",
    "*  Decision trees cannot derive the significance of features, but LR can.\n",
    "*  Decision trees are better for categorical values than LR.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E0AndhW3WTMp"
   },
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "5WG2i6f3owvp",
    "outputId": "3042ef35-3241-4e0f-b079-7ccf5ce2edfd"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(accuracies)\n",
    "df = df.transpose()\n",
    "df = df.reset_index()\n",
    "df = df.rename(columns={'index':'model_name'})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sB3jzc7Uoww3"
   },
   "outputs": [],
   "source": [
    "def visualize_coefficients(classifier, feature_names, n_top_features=25):\n",
    "    # get coefficients with large absolute values \n",
    "    coef = classifier.coef_.ravel()\n",
    "    positive_coefficients = np.argsort(coef)[-n_top_features:]\n",
    "    negative_coefficients = np.argsort(coef)[:n_top_features]\n",
    "    interesting_coefficients = np.hstack([negative_coefficients, positive_coefficients])\n",
    "    # plot them\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    colors = [\"red\" if c < 0 else \"blue\" for c in coef[interesting_coefficients]]\n",
    "    plt.bar(np.arange(50), coef[interesting_coefficients], color=colors)\n",
    "    feature_names = np.array(feature_names)\n",
    "    plt.xticks(np.arange(1, 51), feature_names[interesting_coefficients], rotation=60, ha=\"right\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uOk3FyIeWTMv"
   },
   "source": [
    "# Summary\n",
    "*  We have tried out many different algorithms \n",
    "**  Logistic regression\n",
    "**  Naive bayes\n",
    "**  Random forests \n",
    "*  all the three algorithms have been tested on various hyperparameters and cross validation sets\n",
    "*  In all probablity it is now safe to say that Multinomiaial Naive Bayes algorithm gave us the best output prediction under every circumstance.\n",
    "\n",
    "## Hyperparameters for best score\n",
    "*   NaiveBayes(CV)  :  {'countvectorizer__ngram_range': (1, 3), 'multinomialnb__alpha': 1.0}\n",
    "\n",
    "\n",
    "## Hyperparameters of all algorithms used so far\n",
    "*  LogisticRegresion(TF-IDF)  :  {'logisticregression__C': 1, 'logisticregression__penalty': 'l2', 'tfidfvectorizer__ngram_range': (1, 2)}\n",
    "*  LogisticRegresion(CV)  :  {'countvectorizer__ngram_range': (1, 1), 'logisticregression__C': 0.5, 'logisticregression__penalty': 'l2'}\n",
    "*  NeuralNet(TF-IDF)  :  {'mlpclassifier__activation': 'relu', 'mlpclassifier__hidden_layer_sizes': (64, 128, 256), 'tfidfvectorizer__ngram_range': (1, 2)}\n",
    "*  NeuralNet(CV)  :  {'countvectorizer__ngram_range': (1, 2), 'mlpclassifier__activation': 'logistic', 'mlpclassifier__hidden_layer_sizes': (32, 64, 128)}\n",
    "*  NaiveBayes(TF-IDF)  :  {'multinomialnb__alpha': 0.5, 'tfidfvectorizer__ngram_range': (1, 2)}\n",
    "*  NaiveBayes(CV)  :  {'countvectorizer__ngram_range': (1, 3), 'multinomialnb__alpha': 1.0}\n",
    "*  RandomForest(TF-IDF)  :  {'randomforestclassifier__max_depth': None, 'randomforestclassifier__min_samples_leaf': 1, 'randomforestclassifier__n_estimators': 500, 'tfidfvectorizer__ngram_range': (1, 1)}\n",
    "*  RandomForest(CV)  :  {'countvectorizer__ngram_range': (1, 3), 'randomforestclassifier__max_depth': 5, 'randomforestclassifier__min_samples_leaf': 1, 'randomforestclassifier__n_estimators': 200}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "81ERePb7VxmE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Part+1+Bag+Of+Words+Features+Representation_Complete+Final+sheet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
